"""
Streamlit App - Motorbike Analysis Platform
·ª®ng d·ª•ng web t√≠ch h·ª£p c√°c t√≠nh nƒÉng t·ª´ Project 1 v√† Project 2
"""
import streamlit as st
import pandas as pd
import numpy as np
from pathlib import Path
import sys

# Add project paths to sys.path
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT / "project1"))
sys.path.insert(0, str(PROJECT_ROOT / "project2"))

# Page config
st.set_page_config(
    page_title="Motorbike Analysis Platform",
    page_icon="üèçÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    .stButton>button {
        width: 100%;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state
if 'data_loaded' not in st.session_state:
    st.session_state.data_loaded = False
if 'models_loaded' not in st.session_state:
    st.session_state.models_loaded = False

# Sidebar navigation
st.sidebar.title("üèçÔ∏è Motorbike Analysis")
st.sidebar.markdown("---")

page = st.sidebar.radio(
    "Ch·ªçn ch·ª©c nƒÉng:",
    ["üè† Trang ch·ªß", "üí∞ D·ª± ƒëo√°n gi√°", "üö® Ph√°t hi·ªán b·∫•t th∆∞·ªùng", "üîç G·ª£i √Ω xe t∆∞∆°ng t·ª±", "üìä Ph√¢n c·ª•m d·ªØ li·ªáu"]
)

# Import modules (lazy loading)
@st.cache_resource
def load_price_model():
    """Load price prediction model"""
    try:
        from project1.config import PRICE_MODEL_PATH, PREPROCESSOR_PATH
        import joblib
        
        if not PRICE_MODEL_PATH.exists() or not PREPROCESSOR_PATH.exists():
            return None, None, "Models ch∆∞a ƒë∆∞·ª£c train. Vui l√≤ng ch·∫°y notebooks trong project1/ ƒë·ªÉ t·∫°o models."
        
        # Load model - check if it's a dict or direct model
        model_data = joblib.load(PRICE_MODEL_PATH)
        if isinstance(model_data, dict):
            # Extract model from dict (could be 'model', 'price_model', or direct)
            model = model_data.get('model', model_data.get('price_model', model_data))
            # If still a dict, try to get the actual model object
            if isinstance(model, dict):
                model = model.get('model', model)
        else:
            model = model_data
        
        # Load preprocessor
        preprocessor_data = joblib.load(PREPROCESSOR_PATH)
        if isinstance(preprocessor_data, dict):
            preprocessor = preprocessor_data['preprocessor']
        else:
            preprocessor = preprocessor_data
        
        return model, preprocessor, None
    except Exception as e:
        return None, None, f"L·ªói khi load model: {str(e)}"

@st.cache_resource
def load_anomaly_model():
    """Load anomaly detection model"""
    try:
        from project1.config import ISO_MODEL_PATH, PREPROCESSOR_PATH
        import joblib
        
        if not ISO_MODEL_PATH.exists() or not PREPROCESSOR_PATH.exists():
            return None, None, "Models ch∆∞a ƒë∆∞·ª£c train."
        
        iso_data = joblib.load(ISO_MODEL_PATH)
        # Check if it's a dict (saved with metadata) or direct model
        if isinstance(iso_data, dict):
            iso_model = iso_data.get('model', iso_data.get('iso_model', iso_data))
        else:
            iso_model = iso_data
        
        preprocessor_data = joblib.load(PREPROCESSOR_PATH)
        preprocessor = preprocessor_data['preprocessor']
        return iso_model, preprocessor, None
    except Exception as e:
        return None, None, f"L·ªói khi load model: {str(e)}"

@st.cache_data
def load_sample_data():
    """Load full dataset for recommendation and clustering"""
    try:
        from project2.config import RAW_DATA_FILE, DATA_DIR
        
        # Try multiple paths
        possible_paths = [
            RAW_DATA_FILE,  # data/data_motobikes.xlsx - Sheet1.csv
            PROJECT_ROOT / "data" / "data_motobikes.xlsx - Sheet1.csv",
            PROJECT_ROOT / "project2" / "data_motobikes.xlsx - Sheet1.csv",
            PROJECT_ROOT / "project1" / "data_motobikes.xlsx - Sheet1.csv",
        ]
        
        for path in possible_paths:
            if path.exists():
                # Load FULL dataset, not just sample
                df = pd.read_csv(path, low_memory=False)
                return df, None
        
        return None, f"Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu. ƒê√£ th·ª≠: {[str(p) for p in possible_paths]}"
    except Exception as e:
        return None, f"L·ªói khi load d·ªØ li·ªáu: {str(e)}"

# Home page
if page == "üè† Trang ch·ªß":
    st.markdown('<div class="main-header">üèçÔ∏è Motorbike Analysis Platform</div>', unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("### üí∞ D·ª± ƒëo√°n gi√°")
        st.markdown("""
        D·ª± ƒëo√°n gi√° xe m√°y d·ª±a tr√™n:
        - Th∆∞∆°ng hi·ªáu, d√≤ng xe
        - NƒÉm ƒëƒÉng k√Ω, s·ªë km
        - T√¨nh tr·∫°ng, dung t√≠ch
        """)
    
    with col2:
        st.markdown("### üö® Ph√°t hi·ªán b·∫•t th∆∞·ªùng")
        st.markdown("""
        Ph√°t hi·ªán c√°c tin ƒëƒÉng c√≥ gi√° b·∫•t th∆∞·ªùng:
        - Residual-based detection
        - Isolation Forest
        """)
    
    with col3:
        st.markdown("### üîç G·ª£i √Ω")
        st.markdown("""
        T√¨m xe m√°y t∆∞∆°ng t·ª±:
        - KNN-based recommendation
        - Content-based filtering
        """)
    
    st.markdown("---")
    st.markdown("### üìä Th·ªëng k√™")
    
    # Check models status
    price_model, _, price_err = load_price_model()
    anomaly_model, _, anomaly_err = load_anomaly_model()
    sample_data, data_err = load_sample_data()
    
    status_col1, status_col2, status_col3 = st.columns(3)
    
    with status_col1:
        if price_model:
            st.success("‚úÖ Price Model: S·∫µn s√†ng")
        else:
            st.error(f"‚ùå Price Model: {price_err or 'Ch∆∞a load'}")
    
    with status_col2:
        if anomaly_model:
            st.success("‚úÖ Anomaly Model: S·∫µn s√†ng")
        else:
            st.error(f"‚ùå Anomaly Model: {anomaly_err or 'Ch∆∞a load'}")
    
    with status_col3:
        if sample_data is not None:
            st.success(f"‚úÖ Data: {len(sample_data)} records")
        else:
            st.error(f"‚ùå Data: {data_err or 'Ch∆∞a load'}")

# Price Prediction page
elif page == "üí∞ D·ª± ƒëo√°n gi√°":
    st.title("üí∞ D·ª± ƒëo√°n gi√° xe m√°y")
    st.markdown("Nh·∫≠p th√¥ng tin xe ƒë·ªÉ d·ª± ƒëo√°n gi√°")
    
    model, preprocessor, error = load_price_model()
    
    if error:
        st.error(error)
        st.info("üí° H∆∞·ªõng d·∫´n: Ch·∫°y c√°c notebooks trong project1/ ƒë·ªÉ train models tr∆∞·ªõc.")
    else:
        with st.form("price_prediction_form"):
            col1, col2 = st.columns(2)
            
            with col1:
                thuong_hieu = st.selectbox("Th∆∞∆°ng hi·ªáu", ["Honda", "Yamaha", "SYM", "Piaggio", "Vespa", "Kh√°c"])
                dong_xe = st.text_input("D√≤ng xe", placeholder="V√≠ d·ª•: SH, Air Blade, Exciter")
                nam_dang_ky = st.number_input("NƒÉm ƒëƒÉng k√Ω", min_value=1990, max_value=2024, value=2020)
                so_km = st.number_input("S·ªë km ƒë√£ ƒëi", min_value=0, value=10000)
            
            with col2:
                tinh_trang = st.selectbox("T√¨nh tr·∫°ng", ["M·ªõi", "ƒê√£ s·ª≠ d·ª•ng", "C·∫ßn s·ª≠a ch·ªØa"])
                loai_xe = st.selectbox("Lo·∫°i xe", ["Tay ga", "S·ªë", "Tay c√¥n", "Kh√°c"])
                xuat_xu = st.selectbox("Xu·∫•t x·ª©", ["Vi·ªát Nam", "Th√°i Lan", "Indonesia", "Nh·∫≠t B·∫£n", "Kh√°c"])
                dung_tich_cc = st.number_input("Dung t√≠ch (cc)", min_value=50, max_value=1000, value=125)
            
            tinh_thanh = st.selectbox("T·ªânh/Th√†nh", ["H·ªì Ch√≠ Minh", "H√† N·ªôi", "ƒê√† N·∫µng", "Kh√°c"])
            quan = st.text_input("Qu·∫≠n/Huy·ªán", placeholder="V√≠ d·ª•: Qu·∫≠n 1, Qu·∫≠n 7")
            
            submitted = st.form_submit_button("üîÆ D·ª± ƒëo√°n gi√°", use_container_width=True)
            
            if submitted:
                try:
                    # Get feature names from preprocessor - MUST use exact order
                    from project1.config import PREPROCESSOR_PATH
                    import joblib
                    preprocessor_data = joblib.load(PREPROCESSOR_PATH)
                    if isinstance(preprocessor_data, dict):
                        numeric_features = preprocessor_data.get('numeric_features', [])
                        categorical_features = preprocessor_data.get('categorical_features', [])
                    else:
                        # Fallback: use default feature names
                        numeric_features = ['so_km', 'nam_dang_ky', 'dung_tich_cc', 'trong_luong_kg', 'len_title', 'len_desc']
                        categorical_features = ['thuong_hieu', 'dong_xe', 'tinh_trang', 'loai_xe', 'xuat_xu', 'tinh_thanh', 'quan']
                    
                    # CRITICAL: Use exact feature order that preprocessor expects
                    all_features = numeric_features + categorical_features
                    
                    # Prepare input data - must match exact column names and order
                    input_data = pd.DataFrame({
                        'so_km': [so_km],
                        'nam_dang_ky': [nam_dang_ky],
                        'dung_tich_cc': [dung_tich_cc],
                        'trong_luong_kg': [np.nan],
                        'len_title': [len(dong_xe) if dong_xe else 0],
                        'len_desc': [0],
                        'thuong_hieu': [thuong_hieu],
                        'dong_xe': [dong_xe if dong_xe else ""],
                        'tinh_trang': [tinh_trang],
                        'loai_xe': [loai_xe],
                        'xuat_xu': [xuat_xu],
                        'tinh_thanh': [tinh_thanh],
                        'quan': [quan if quan else ""]
                    }, columns=all_features)  # Ensure correct column order
                    
                    # Check if model is a Pipeline (contains preprocessor)
                    from sklearn.pipeline import Pipeline
                    is_pipeline = isinstance(model, Pipeline) or (hasattr(model, 'steps') and len(model.steps) > 0)
                    
                    if is_pipeline:
                        # Model already includes preprocessor, use raw input (13 features)
                        prediction = model.predict(input_data)[0]
                    else:
                        # Model needs transformed input (278 features)
                        X_transformed = preprocessor.transform(input_data)
                        
                        # Handle sparse matrix
                        if hasattr(X_transformed, 'toarray'):
                            X_transformed = X_transformed.toarray()
                        
                        prediction = model.predict(X_transformed)[0]
                    
                    # Validate prediction
                    if prediction <= 0 or np.isnan(prediction) or np.isinf(prediction):
                        st.warning("‚ö†Ô∏è Gi√° d·ª± ƒëo√°n kh√¥ng h·ª£p l·ªá. Vui l√≤ng ki·ªÉm tra l·∫°i th√¥ng tin ƒë·∫ßu v√†o.")
                    else:
                        # Display result
                        st.success(f"### üí∞ Gi√° d·ª± ƒëo√°n: {prediction:,.0f} VNƒê")
                        st.info(f"‚âà {prediction/1_000_000:.2f} tri·ªáu VNƒê")
                    
                except Exception as e:
                    st.error(f"L·ªói khi d·ª± ƒëo√°n: {str(e)}")
                    import traceback
                    with st.expander("Chi ti·∫øt l·ªói"):
                        st.code(traceback.format_exc())

# Anomaly Detection page
elif page == "üö® Ph√°t hi·ªán b·∫•t th∆∞·ªùng":
    st.title("üö® Ph√°t hi·ªán gi√° b·∫•t th∆∞·ªùng")
    st.markdown("Ki·ªÉm tra xem gi√° xe c√≥ b·∫•t th∆∞·ªùng so v·ªõi th·ªã tr∆∞·ªùng kh√¥ng")
    
    model, preprocessor, error = load_anomaly_model()
    
    if error:
        st.error(error)
    else:
        st.info("Nh·∫≠p th√¥ng tin xe v√† gi√° ƒë·ªÉ ki·ªÉm tra")
        
        with st.form("anomaly_detection_form"):
            col1, col2 = st.columns(2)
            
            with col1:
                thuong_hieu = st.selectbox("Th∆∞∆°ng hi·ªáu", ["Honda", "Yamaha", "SYM", "Piaggio", "Vespa"])
                dong_xe = st.text_input("D√≤ng xe")
                nam_dang_ky = st.number_input("NƒÉm ƒëƒÉng k√Ω", min_value=1990, max_value=2024)
                so_km = st.number_input("S·ªë km", min_value=0)
            
            with col2:
                tinh_trang = st.selectbox("T√¨nh tr·∫°ng", ["M·ªõi", "ƒê√£ s·ª≠ d·ª•ng", "C·∫ßn s·ª≠a ch·ªØa"])
                loai_xe = st.selectbox("Lo·∫°i xe", ["Tay ga", "S·ªë", "Tay c√¥n"])
                dung_tich_cc = st.number_input("Dung t√≠ch (cc)", min_value=50, max_value=1000)
                gia_vnd = st.number_input("Gi√° (VNƒê)", min_value=0, format="%d")
            
            submitted = st.form_submit_button("üîç Ki·ªÉm tra", use_container_width=True)
            
            if submitted:
                try:
                    # Get feature names - MUST use exact order
                    from project1.config import PREPROCESSOR_PATH
                    import joblib
                    preprocessor_data = joblib.load(PREPROCESSOR_PATH)
                    if isinstance(preprocessor_data, dict):
                        numeric_features = preprocessor_data.get('numeric_features', [])
                        categorical_features = preprocessor_data.get('categorical_features', [])
                    else:
                        numeric_features = ['so_km', 'nam_dang_ky', 'dung_tich_cc', 'trong_luong_kg', 'len_title', 'len_desc']
                        categorical_features = ['thuong_hieu', 'dong_xe', 'tinh_trang', 'loai_xe', 'xuat_xu', 'tinh_thanh', 'quan']
                    
                    # CRITICAL: Use exact feature order that preprocessor expects
                    all_features = numeric_features + categorical_features
                    
                    # Prepare input with correct columns and order
                    input_data = pd.DataFrame({
                        'so_km': [so_km],
                        'nam_dang_ky': [nam_dang_ky],
                        'dung_tich_cc': [dung_tich_cc],
                        'trong_luong_kg': [np.nan],
                        'len_title': [len(dong_xe) if dong_xe else 0],
                        'len_desc': [0],
                        'thuong_hieu': [thuong_hieu],
                        'dong_xe': [dong_xe if dong_xe else ""],
                        'tinh_trang': [tinh_trang],
                        'loai_xe': [loai_xe],
                        'xuat_xu': ["Vi·ªát Nam"],
                        'tinh_thanh': ["H·ªì Ch√≠ Minh"],
                        'quan': [""]
                    }, columns=all_features)  # Ensure correct column order
                    
                    # Transform
                    X_transformed = preprocessor.transform(input_data)
                    
                    # Handle sparse matrix
                    if hasattr(X_transformed, 'toarray'):
                        X_transformed = X_transformed.toarray()
                    
                    # Predict anomaly
                    anomaly_score = model.decision_function(X_transformed)[0]
                    predictions = model.predict(X_transformed)
                    is_anomaly = predictions[0] == -1
                    
                    # Validate scores
                    if np.isnan(anomaly_score) or np.isinf(anomaly_score):
                        st.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ t√≠nh anomaly score. Vui l√≤ng ki·ªÉm tra l·∫°i th√¥ng tin.")
                    else:
                        # Display result
                        if is_anomaly:
                            st.error("### ‚ö†Ô∏è Ph√°t hi·ªán gi√° B·∫§T TH∆Ø·ªúNG")
                            st.warning(f"Anomaly score: {anomaly_score:.4f}")
                            st.info("Gi√° n√†y c√≥ v·∫ª kh√¥ng ph√π h·ª£p v·ªõi th·ªã tr∆∞·ªùng. N√™n ki·ªÉm tra l·∫°i.")
                            
                            # Show predicted price for comparison
                            try:
                                price_model, _, _ = load_price_model()
                                if price_model is not None:
                                    price_pred = price_model.predict(X_transformed)[0]
                                    if price_pred > 0:
                                        st.info(f"üí° Gi√° d·ª± ƒëo√°n h·ª£p l√Ω: {price_pred/1_000_000:.2f} tri·ªáu VNƒê")
                                        st.info(f"üí° Gi√° b·∫°n nh·∫≠p: {gia_vnd/1_000_000:.2f} tri·ªáu VNƒê")
                                        diff_pct = abs(price_pred - gia_vnd) / price_pred * 100
                                        if diff_pct > 30:
                                            st.warning(f"‚ö†Ô∏è Ch√™nh l·ªách {diff_pct:.1f}% so v·ªõi gi√° d·ª± ƒëo√°n - ƒë√¢y l√† l√Ω do ph√°t hi·ªán b·∫•t th∆∞·ªùng")
                            except Exception as e:
                                # Silently fail - not critical
                                pass
                        else:
                            st.success("### ‚úÖ Gi√° B√åNH TH∆Ø·ªúNG")
                            st.info(f"Anomaly score: {anomaly_score:.4f}")
                            st.success("Gi√° n√†y ph√π h·ª£p v·ªõi th·ªã tr∆∞·ªùng.")
                        
                except Exception as e:
                    st.error(f"L·ªói: {str(e)}")
                    import traceback
                    with st.expander("Chi ti·∫øt l·ªói"):
                        st.code(traceback.format_exc())

# Recommendation page
elif page == "üîç G·ª£i √Ω xe t∆∞∆°ng t·ª±":
    st.title("üîç T√¨m xe m√°y t∆∞∆°ng t·ª±")
    st.markdown("Nh·∫≠p ID ho·∫∑c th√¥ng tin xe ƒë·ªÉ t√¨m c√°c xe t∆∞∆°ng t·ª±")
    
    sample_data, error = load_sample_data()
    
    if error:
        st.error(error)
    else:
        st.success(f"üìä ƒê√£ load **{len(sample_data):,}** records t·ª´ dataset Ch·ª£ T·ªët (to√†n b·ªô d·ªØ li·ªáu)")
        
        # Import utils
        from utils import get_bike_info, find_similar_bikes, format_price, parse_price
        
        # Simple recommendation interface
        st.subheader("üîé T√¨m ki·∫øm")
        search_option = st.radio("T√¨m theo:", ["ID", "Th∆∞∆°ng hi·ªáu", "D√≤ng xe", "Th√¥ng tin t√πy ch·ªânh"], horizontal=True)
        
        if search_option == "ID":
            col1, col2 = st.columns([3, 1])
            with col1:
                bike_id = st.text_input("Nh·∫≠p ID xe", placeholder="V√≠ d·ª•: 12345")
            with col2:
                top_n = st.number_input("S·ªë k·∫øt qu·∫£", min_value=1, max_value=20, value=5)
            
            if bike_id and st.button("üîç T√¨m xe t∆∞∆°ng t·ª±", use_container_width=True):
                bike_info = get_bike_info(sample_data, bike_id)
                if bike_info:
                    st.success(f"‚úÖ T√¨m th·∫•y xe: {bike_info.get('Ti√™u ƒë·ªÅ', bike_info.get('tieu_de', 'N/A'))}")
                    
                    # Show bike info
                    with st.expander("üìã Th√¥ng tin xe", expanded=False):
                        info_cols = st.columns(3)
                        with info_cols[0]:
                            st.metric("Th∆∞∆°ng hi·ªáu", bike_info.get('Th∆∞∆°ng hi·ªáu', bike_info.get('thuong_hieu', 'N/A')))
                        with info_cols[1]:
                            price = parse_price(bike_info.get('Gi√°', bike_info.get('gia_vnd', None)))
                            st.metric("Gi√°", format_price(price))
                        with info_cols[2]:
                            st.metric("NƒÉm", bike_info.get('NƒÉm ƒëƒÉng k√Ω', bike_info.get('nam_dang_ky', 'N/A')))
                    
                    # Find similar
                    similar = find_similar_bikes(bike_info, sample_data, top_n=top_n)
                    
                    if similar:
                        st.subheader(f"üéØ {len(similar)} xe t∆∞∆°ng t·ª±")
                        for i, bike in enumerate(similar, 1):
                            with st.container():
                                cols = st.columns([1, 2, 1, 1])
                                with cols[0]:
                                    st.write(f"**#{i}**")
                                with cols[1]:
                                    title = bike.get('Ti√™u ƒë·ªÅ', bike.get('tieu_de', 'N/A'))
                                    st.write(f"**{title}**")
                                with cols[2]:
                                    price = parse_price(bike.get('Gi√°', bike.get('gia_vnd', None)))
                                    st.write(format_price(price))
                                with cols[3]:
                                    st.write(bike.get('Th∆∞∆°ng hi·ªáu', bike.get('thuong_hieu', 'N/A')))
                                st.divider()
                    else:
                        st.warning("Kh√¥ng t√¨m th·∫•y xe t∆∞∆°ng t·ª±")
                else:
                    st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y xe v·ªõi ID: {bike_id}")
                    st.info("üí° Th·ª≠ t√¨m theo th∆∞∆°ng hi·ªáu ho·∫∑c d√≤ng xe")
        
        elif search_option == "Th∆∞∆°ng hi·ªáu":
            brands = sorted(sample_data['Th∆∞∆°ng hi·ªáu'].dropna().unique()) if 'Th∆∞∆°ng hi·ªáu' in sample_data.columns else []
            if brands:
                selected_brand = st.selectbox("Ch·ªçn th∆∞∆°ng hi·ªáu", brands)
                if st.button("üîç T√¨m", use_container_width=True):
                    filtered = sample_data[sample_data['Th∆∞∆°ng hi·ªáu'] == selected_brand]
                    st.subheader(f"üìä T√¨m th·∫•y {len(filtered)} xe {selected_brand}")
                    st.dataframe(
                        filtered[['Ti√™u ƒë·ªÅ', 'Gi√°', 'NƒÉm ƒëƒÉng k√Ω', 'S·ªë Km ƒë√£ ƒëi']].head(20),
                        use_container_width=True,
                        hide_index=True
                    )
            else:
                st.warning("Kh√¥ng c√≥ c·ªôt 'Th∆∞∆°ng hi·ªáu' trong d·ªØ li·ªáu")
        
        elif search_option == "D√≤ng xe":
            model_name = st.text_input("Nh·∫≠p t√™n d√≤ng xe", placeholder="V√≠ d·ª•: SH, Air Blade")
            if model_name and st.button("üîç T√¨m", use_container_width=True):
                if 'D√≤ng xe' in sample_data.columns:
                    filtered = sample_data[sample_data['D√≤ng xe'].str.contains(model_name, case=False, na=False)]
                    st.subheader(f"üìä T√¨m th·∫•y {len(filtered)} xe")
                    st.dataframe(
                        filtered[['Ti√™u ƒë·ªÅ', 'Gi√°', 'Th∆∞∆°ng hi·ªáu', 'NƒÉm ƒëƒÉng k√Ω']].head(20),
                        use_container_width=True,
                        hide_index=True
                    )
                else:
                    st.warning("Kh√¥ng c√≥ c·ªôt 'D√≤ng xe' trong d·ªØ li·ªáu")
        
        elif search_option == "Th√¥ng tin t√πy ch·ªânh":
            st.subheader("üîß T√¨m ki·∫øm n√¢ng cao")
            col1, col2 = st.columns(2)
            with col1:
                brand = st.selectbox("Th∆∞∆°ng hi·ªáu", ["T·∫•t c·∫£"] + sorted(sample_data['Th∆∞∆°ng hi·ªáu'].dropna().unique().tolist()) if 'Th∆∞∆°ng hi·ªáu' in sample_data.columns else ["T·∫•t c·∫£"])
                min_price = st.number_input("Gi√° t·ªëi thi·ªÉu (tri·ªáu)", min_value=0, value=0)
                max_price = st.number_input("Gi√° t·ªëi ƒëa (tri·ªáu)", min_value=0, value=500)
            with col2:
                min_year = st.number_input("NƒÉm t·ªëi thi·ªÉu", min_value=1990, max_value=2024, value=2010)
                max_year = st.number_input("NƒÉm t·ªëi ƒëa", min_value=1990, max_value=2024, value=2024)
            
            if st.button("üîç T√¨m ki·∫øm", use_container_width=True):
                filtered = sample_data.copy()
                
                # Filter by brand
                if brand != "T·∫•t c·∫£" and 'Th∆∞∆°ng hi·ªáu' in filtered.columns:
                    filtered = filtered[filtered['Th∆∞∆°ng hi·ªáu'] == brand]
                
                # Filter by price
                if 'Gi√°' in filtered.columns:
                    from utils import parse_price
                    filtered['price_parsed'] = filtered['Gi√°'].apply(parse_price)
                    filtered = filtered[(filtered['price_parsed'] >= min_price) & (filtered['price_parsed'] <= max_price)]
                
                # Filter by year - parse year first
                if 'NƒÉm ƒëƒÉng k√Ω' in filtered.columns:
                    def safe_parse_year_for_filter(value):
                        if pd.isna(value):
                            return None
                        try:
                            import re
                            value_str = str(value).strip()
                            year_match = re.search(r'\d{4}', value_str)
                            if year_match:
                                year = int(year_match.group())
                                if 1990 <= year <= 2025:
                                    return year
                            return None
                        except:
                            return None
                    
                    # Parse years and filter
                    filtered['year_parsed'] = filtered['NƒÉm ƒëƒÉng k√Ω'].apply(safe_parse_year_for_filter)
                    filtered = filtered[
                        (filtered['year_parsed'].notna()) & 
                        (filtered['year_parsed'] >= min_year) & 
                        (filtered['year_parsed'] <= max_year)
                    ]
                    # Drop temporary column
                    if 'year_parsed' in filtered.columns:
                        filtered = filtered.drop(columns=['year_parsed'])
                
                st.subheader(f"üìä T√¨m th·∫•y {len(filtered)} xe ph√π h·ª£p")
                if len(filtered) > 0:
                    display_cols = ['Ti√™u ƒë·ªÅ', 'Gi√°', 'Th∆∞∆°ng hi·ªáu', 'NƒÉm ƒëƒÉng k√Ω']
                    available_cols = [col for col in display_cols if col in filtered.columns]
                    st.dataframe(
                        filtered[available_cols].head(50),
                        use_container_width=True,
                        hide_index=True
                    )
                else:
                    st.info("Kh√¥ng t√¨m th·∫•y xe ph√π h·ª£p v·ªõi ti√™u ch√≠")

# Clustering page
elif page == "üìä Ph√¢n c·ª•m d·ªØ li·ªáu":
    st.title("üìä Ph√¢n c·ª•m d·ªØ li·ªáu - Ph√¢n kh√∫c th·ªã tr∆∞·ªùng xe m√°y")
    st.markdown("""
    **Ph√¢n c·ª•m d·ªØ li·ªáu gi√∫p:**
    - üéØ Ph√¢n kh√∫c th·ªã tr∆∞·ªùng: Chia xe m√°y th√†nh c√°c nh√≥m c√≥ ƒë·∫∑c ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng
    - üë• Hi·ªÉu kh√°ch h√†ng: M·ªói ph√¢n kh√∫c ƒë·∫°i di·ªán cho m·ªôt nh√≥m kh√°ch h√†ng kh√°c nhau
    - üí∞ ƒê·ªãnh gi√° h·ª£p l√Ω: Bi·∫øt xe thu·ªôc ph√¢n kh√∫c n√†o ƒë·ªÉ ƒë·ªãnh gi√° ph√π h·ª£p
    - üîç G·ª£i √Ω s·∫£n ph·∫©m: ƒê·ªÅ xu·∫•t xe t∆∞∆°ng t·ª± trong c√πng ph√¢n kh√∫c
    """)
    
    sample_data, data_error = load_sample_data()
    
    if data_error:
        st.error(data_error)
    else:
        st.success(f"üìä ƒê√£ load **{len(sample_data):,}** records t·ª´ dataset Ch·ª£ T·ªët (to√†n b·ªô d·ªØ li·ªáu)")
        
        # Import clustering functions
        from sklearn.cluster import KMeans, AgglomerativeClustering
        from sklearn.mixture import GaussianMixture
        from sklearn.preprocessing import StandardScaler
        from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        tab1, tab2, tab3 = st.tabs(["üîç Clustering", "üìä Content-Based Filtering", "üìà Visualization"])
        
        with tab1:
            st.subheader("üîç Ph√¢n c·ª•m d·ªØ li·ªáu")
            
            # Prepare data for clustering
            @st.cache_data
            def prepare_clustering_data(df):
                """Prepare numeric features for clustering"""
                try:
                    if df is None or len(df) == 0:
                        return None, None, None
                    
                    df_clean = df.copy()
                    numeric_cols = []
                    
                    # Parse price
                    from utils import parse_price
                    if 'Gi√°' in df_clean.columns:
                        df_clean['price_parsed'] = df_clean['Gi√°'].apply(lambda x: parse_price(x) if pd.notna(x) else 0)
                        numeric_cols.append('price_parsed')
                    elif 'gia_vnd' in df_clean.columns:
                        df_clean['price_parsed'] = df_clean['gia_vnd'] / 1_000_000
                        numeric_cols.append('price_parsed')
                    
                    # Year - parse carefully to handle strings like "2008 tr∆∞·ªõc nƒÉm"
                    def parse_year(value):
                        if pd.isna(value):
                            return 0
                        try:
                            # Convert to string first
                            value_str = str(value).strip()
                            # Extract first 4 digits (year)
                            import re
                            year_match = re.search(r'\d{4}', value_str)
                            if year_match:
                                year = int(year_match.group())
                                # Validate year range
                                if 1990 <= year <= 2025:
                                    return year
                            return 0
                        except:
                            return 0
                    
                    if 'NƒÉm ƒëƒÉng k√Ω' in df_clean.columns:
                        df_clean['year_parsed'] = df_clean['NƒÉm ƒëƒÉng k√Ω'].apply(parse_year)
                        numeric_cols.append('year_parsed')
                    elif 'nam_dang_ky' in df_clean.columns:
                        df_clean['year_parsed'] = df_clean['nam_dang_ky'].apply(parse_year)
                        numeric_cols.append('year_parsed')
                    
                    # KM - parse carefully
                    def parse_km(value):
                        if pd.isna(value):
                            return 0
                        try:
                            # Convert to string and extract numbers
                            value_str = str(value).strip().lower()
                            # Remove common text
                            value_str = value_str.replace('km', '').replace(',', '').replace('.', '').strip()
                            # Extract numbers
                            import re
                            numbers = re.findall(r'\d+', value_str)
                            if numbers:
                                return float(''.join(numbers))
                            return 0
                        except:
                            return 0
                    
                    if 'S·ªë Km ƒë√£ ƒëi' in df_clean.columns:
                        df_clean['km_parsed'] = df_clean['S·ªë Km ƒë√£ ƒëi'].apply(parse_km)
                        numeric_cols.append('km_parsed')
                    elif 'so_km' in df_clean.columns:
                        df_clean['km_parsed'] = df_clean['so_km'].apply(parse_km)
                        numeric_cols.append('km_parsed')
                    
                    # One-hot encode brand
                    brand_col = None
                    if 'Th∆∞∆°ng hi·ªáu' in df_clean.columns:
                        brand_col = 'Th∆∞∆°ng hi·ªáu'
                    elif 'thuong_hieu' in df_clean.columns:
                        brand_col = 'thuong_hieu'
                    
                    if brand_col and df_clean[brand_col].notna().sum() > 0:
                        # Limit to top brands to avoid too many features
                        top_brands = df_clean[brand_col].value_counts().head(10).index.tolist()
                        for brand in top_brands:
                            col_name = f'brand_{brand}'
                            df_clean[col_name] = (df_clean[brand_col] == brand).astype(int)
                            numeric_cols.append(col_name)
                    
                    # Check if we have enough features
                    if len(numeric_cols) == 0:
                        st.warning("Kh√¥ng t√¨m th·∫•y c·ªôt s·ªë ph√π h·ª£p. Th√™m c√°c c·ªôt m·∫∑c ƒë·ªãnh.")
                        # Add dummy features
                        df_clean['dummy_feature'] = 1
                        numeric_cols.append('dummy_feature')
                    
                    # Select and clean
                    available_cols = [col for col in numeric_cols if col in df_clean.columns]
                    if len(available_cols) == 0:
                        return None, None, None
                    
                    X = df_clean[available_cols].fillna(0)
                    
                    # Remove rows with all zeros
                    X = X[(X != 0).any(axis=1)]
                    if len(X) == 0:
                        return None, None, None
                    
                    # Scale
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)
                    
                    # Update df_clean to match X indices
                    df_clean = df_clean.loc[X.index].copy()
                    
                    return X_scaled, df_clean, scaler
                except Exception as e:
                    st.error(f"L·ªói khi chu·∫©n b·ªã d·ªØ li·ªáu: {str(e)}")
                    import traceback
                    st.code(traceback.format_exc())
                    return None, None, None
            
            X_scaled, df_clean, scaler = prepare_clustering_data(sample_data)
            
            if X_scaled is not None:
                col1, col2 = st.columns(2)
                
                with col1:
                    n_clusters = st.slider("S·ªë c·ª•m (k)", min_value=2, max_value=10, value=5)
                    algorithm = st.selectbox(
                        "Thu·∫≠t to√°n clustering",
                        ["KMeans", "Gaussian Mixture Model (GMM)", "Agglomerative Clustering"]
                    )
                
                with col2:
                    max_samples = st.slider("S·ªë m·∫´u t·ªëi ƒëa", min_value=100, max_value=min(1000, len(sample_data)), value=min(500, len(sample_data)))
                
                if st.button("üöÄ Ch·∫°y Clustering", use_container_width=True):
                    with st.spinner("ƒêang ch·∫°y clustering..."):
                        # Sample data if too large
                        if len(X_scaled) > max_samples:
                            indices = np.random.choice(len(X_scaled), max_samples, replace=False)
                            X_sample = X_scaled[indices]
                            df_sample = df_clean.iloc[indices].copy()
                        else:
                            X_sample = X_scaled
                            df_sample = df_clean.copy()
                        
                        # Run clustering
                        if algorithm == "KMeans":
                            model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                            labels = model.fit_predict(X_sample)
                        elif algorithm == "Gaussian Mixture Model (GMM)":
                            model = GaussianMixture(n_components=n_clusters, random_state=42)
                            labels = model.fit_predict(X_sample)
                        else:  # Agglomerative
                            model = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
                            labels = model.fit_predict(X_sample)
                        
                        # Calculate metrics
                        if len(np.unique(labels)) >= 2:
                            sil_score = silhouette_score(X_sample, labels)
                            db_score = davies_bouldin_score(X_sample, labels)
                            ch_score = calinski_harabasz_score(X_sample, labels)
                        else:
                            sil_score = db_score = ch_score = np.nan
                        
                        # Display results
                        st.success(f"‚úÖ Ho√†n th√†nh clustering v·ªõi {algorithm}")
                        
                        # Metrics
                        metric_col1, metric_col2, metric_col3 = st.columns(3)
                        with metric_col1:
                            st.metric("Silhouette Score", f"{sil_score:.4f}" if not np.isnan(sil_score) else "N/A")
                        with metric_col2:
                            st.metric("Davies-Bouldin Score", f"{db_score:.4f}" if not np.isnan(db_score) else "N/A")
                        with metric_col3:
                            st.metric("Calinski-Harabasz Score", f"{ch_score:.4f}" if not np.isnan(ch_score) else "N/A")
                        
                        # Cluster summary
                        df_sample['cluster'] = labels
                        
                        # Store in session state for visualization
                        st.session_state['cluster_labels'] = labels
                        st.session_state['cluster_data'] = df_sample
                        st.session_state['cluster_X'] = X_sample
                        
                        st.subheader("üìä T√≥m t·∫Øt c√°c c·ª•m")
                        
                        cluster_summary = []
                        for cluster_id in range(n_clusters):
                            cluster_data = df_sample[df_sample['cluster'] == cluster_id]
                            if len(cluster_data) > 0:
                                # Get prices
                                if 'price_parsed' in cluster_data.columns:
                                    prices = cluster_data['price_parsed'].dropna()
                                elif 'Gi√°' in cluster_data.columns:
                                    from utils import parse_price
                                    prices = cluster_data['Gi√°'].apply(parse_price).dropna()
                                else:
                                    prices = pd.Series()
                                
                                # Get years (use parsed year if available)
                                if 'year_parsed' in cluster_data.columns:
                                    years = cluster_data['year_parsed'].dropna()
                                    years = years[years > 0]  # Remove invalid years
                                elif 'NƒÉm ƒëƒÉng k√Ω' in cluster_data.columns:
                                    def safe_parse_year(v):
                                        try:
                                            import re
                                            if pd.isna(v):
                                                return None
                                            match = re.search(r'\d{4}', str(v))
                                            if match:
                                                y = int(match.group())
                                                return y if 1990 <= y <= 2025 else None
                                            return None
                                        except:
                                            return None
                                    years = cluster_data['NƒÉm ƒëƒÉng k√Ω'].apply(safe_parse_year).dropna()
                                else:
                                    years = pd.Series()
                                
                                # Get brands
                                brand_counts = cluster_data['Th∆∞∆°ng hi·ªáu'].value_counts().head(3) if 'Th∆∞∆°ng hi·ªáu' in cluster_data.columns else {}
                                
                                cluster_summary.append({
                                    'C·ª•m': cluster_id,
                                    'S·ªë l∆∞·ª£ng': len(cluster_data),
                                    'Gi√° TB (tri·ªáu)': f"{prices.mean():.2f}" if len(prices) > 0 else "N/A",
                                    'NƒÉm TB': f"{years.mean():.0f}" if len(years) > 0 else "N/A",
                                    'Th∆∞∆°ng hi·ªáu ph·ªï bi·∫øn': ", ".join(brand_counts.index.tolist()[:3]) if len(brand_counts) > 0 else "N/A"
                                })
                        
                        summary_df = pd.DataFrame(cluster_summary)
                        st.dataframe(summary_df, use_container_width=True, hide_index=True)
                        
                        # Cluster insights and recommendations
                        st.subheader("üí° Ph√¢n t√≠ch v√† G·ª£i √Ω cho t·ª´ng ph√¢n kh√∫c")
                        
                        for cluster_id in range(n_clusters):
                            cluster_data = df_sample[df_sample['cluster'] == cluster_id]
                            if len(cluster_data) > 0:
                                with st.expander(f"üìä Ph√¢n kh√∫c {cluster_id} - {len(cluster_data)} xe", expanded=(cluster_id == 0)):
                                    # Get statistics
                                    if 'price_parsed' in cluster_data.columns:
                                        prices = cluster_data['price_parsed'].dropna()
                                    elif 'Gi√°' in cluster_data.columns:
                                        from utils import parse_price
                                        prices = cluster_data['Gi√°'].apply(parse_price).dropna()
                                    else:
                                        prices = pd.Series()
                                    
                                    if 'year_parsed' in cluster_data.columns:
                                        years = cluster_data['year_parsed'].dropna()
                                        years = years[years > 0]
                                    elif 'NƒÉm ƒëƒÉng k√Ω' in cluster_data.columns:
                                        def safe_parse_year(v):
                                            try:
                                                import re
                                                if pd.isna(v):
                                                    return None
                                                match = re.search(r'\d{4}', str(v))
                                                if match:
                                                    y = int(match.group())
                                                    return y if 1990 <= y <= 2025 else None
                                                return None
                                            except:
                                                return None
                                        years = cluster_data['NƒÉm ƒëƒÉng k√Ω'].apply(safe_parse_year).dropna()
                                    else:
                                        years = pd.Series()
                                    
                                    brand_counts = cluster_data['Th∆∞∆°ng hi·ªáu'].value_counts().head(5) if 'Th∆∞∆°ng hi·ªáu' in cluster_data.columns else {}
                                    
                                    # Display insights
                                    col1, col2, col3 = st.columns(3)
                                    with col1:
                                        if len(prices) > 0:
                                            avg_price = prices.mean()
                                            st.metric("üí∞ Gi√° trung b√¨nh", f"{avg_price:.1f} tri·ªáu VNƒê")
                                            st.caption(f"Kho·∫£ng: {prices.min():.1f} - {prices.max():.1f} tri·ªáu")
                                    with col2:
                                        if len(years) > 0:
                                            avg_year = years.mean()
                                            st.metric("üìÖ NƒÉm trung b√¨nh", f"{avg_year:.0f}")
                                            st.caption(f"Kho·∫£ng: {years.min():.0f} - {years.max():.0f}")
                                    with col3:
                                        st.metric("üìä S·ªë l∆∞·ª£ng", f"{len(cluster_data)} xe")
                                        st.caption(f"T·ª∑ l·ªá: {len(cluster_data)/len(df_sample)*100:.1f}%")
                                    
                                    # Brand distribution
                                    if len(brand_counts) > 0:
                                        st.markdown("**üèçÔ∏è Th∆∞∆°ng hi·ªáu ph·ªï bi·∫øn:**")
                                        brand_text = ", ".join([f"{brand} ({count})" for brand, count in brand_counts.items()])
                                        st.info(brand_text)
                                    
                                    # Characterize cluster
                                    st.markdown("**üéØ ƒê·∫∑c ƒëi·ªÉm ph√¢n kh√∫c:**")
                                    if len(prices) > 0 and len(years) > 0:
                                        avg_price_val = prices.mean()
                                        avg_year_val = years.mean()
                                        
                                        # Determine segment
                                        if avg_price_val < 20:
                                            segment = "üí∞ **Ph√¢n kh√∫c gi√° r·∫ª** - Ph√π h·ª£p cho ng∆∞·ªùi mua c√≥ ng√¢n s√°ch h·∫°n ch·∫ø"
                                        elif avg_price_val < 40:
                                            segment = "üè† **Ph√¢n kh√∫c t·∫ßm trung** - Ph√π h·ª£p cho ng∆∞·ªùi d√πng ph·ªï th√¥ng"
                                        elif avg_price_val < 70:
                                            segment = "‚≠ê **Ph√¢n kh√∫c cao c·∫•p** - Ph√π h·ª£p cho ng∆∞·ªùi d√πng c√≥ thu nh·∫≠p t·ªët"
                                        else:
                                            segment = "üíé **Ph√¢n kh√∫c si√™u cao c·∫•p** - Ph√π h·ª£p cho ng∆∞·ªùi d√πng cao c·∫•p"
                                        
                                        st.markdown(segment)
                                        
                                        # Recommendations
                                        st.markdown("**üí° G·ª£i √Ω:**")
                                        if avg_year_val >= 2020:
                                            st.info("‚úÖ Xe m·ªõi, ph√π h·ª£p cho ng∆∞·ªùi mu·ªën xe ƒë·ªùi m·ªõi, √≠t ph·∫£i s·ª≠a ch·ªØa")
                                        elif avg_year_val >= 2015:
                                            st.info("‚úÖ Xe ƒë·ªùi trung, c√¢n b·∫±ng gi·ªØa gi√° v√† ch·∫•t l∆∞·ª£ng")
                                        else:
                                            st.info("‚úÖ Xe ƒë·ªùi c≈©, gi√° r·∫ª nh∆∞ng c·∫ßn ki·ªÉm tra k·ªπ tr∆∞·ªõc khi mua")
                                    
                                    # Show samples
                                    st.markdown("**üîç M·∫´u xe trong ph√¢n kh√∫c:**")
                                    display_cols = ['Ti√™u ƒë·ªÅ', 'Gi√°', 'Th∆∞∆°ng hi·ªáu', 'NƒÉm ƒëƒÉng k√Ω']
                                    available_cols = [col for col in display_cols if col in cluster_data.columns]
                                    st.dataframe(
                                        cluster_data[available_cols].head(10),
                                        use_container_width=True,
                                        hide_index=True
                                    )
            else:
                st.error("Kh√¥ng th·ªÉ chu·∫©n b·ªã d·ªØ li·ªáu cho clustering. Ki·ªÉm tra l·∫°i d·ªØ li·ªáu.")
        
        with tab2:
            st.subheader("üìä Content-Based Filtering")
            st.markdown("T√¨m xe t∆∞∆°ng t·ª± d·ª±a tr√™n n·ªôi dung (th∆∞∆°ng hi·ªáu, gi√°, nƒÉm, s·ªë km)")
            
            if sample_data is not None and len(sample_data) > 0:
                # Choose input method
                input_method = st.radio(
                    "C√°ch nh·∫≠p th√¥ng tin:",
                    ["üìù Nh·∫≠p th√¥ng tin tr·ª±c ti·∫øp", "üîç Ch·ªçn t·ª´ danh s√°ch xe"],
                    horizontal=True
                )
                
                selected_bike = None
                
                if input_method == "üìù Nh·∫≠p th√¥ng tin tr·ª±c ti·∫øp":
                    st.markdown("### Nh·∫≠p th√¥ng tin xe")
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        cbf_thuong_hieu = st.selectbox(
                            "Th∆∞∆°ng hi·ªáu",
                            ["T·∫•t c·∫£"] + sorted(sample_data['Th∆∞∆°ng hi·ªáu'].dropna().unique().tolist()) if 'Th∆∞∆°ng hi·ªáu' in sample_data.columns else ["T·∫•t c·∫£"],
                            key="cbf_brand"
                        )
                        cbf_gia = st.number_input(
                            "Gi√° (tri·ªáu VNƒê)",
                            min_value=0.0,
                            max_value=1000.0,
                            value=50.0,
                            step=1.0,
                            key="cbf_price"
                        )
                    
                    with col2:
                        cbf_nam = st.number_input(
                            "NƒÉm s·∫£n xu·∫•t",
                            min_value=1990,
                            max_value=2024,
                            value=2020,
                            key="cbf_year"
                        )
                        cbf_km = st.number_input(
                            "S·ªë km ƒë√£ ƒëi",
                            min_value=0,
                            value=10000,
                            key="cbf_km"
                        )
                    
                    # Create a dummy bike dict for similarity calculation
                    selected_bike = {
                        'Th∆∞∆°ng hi·ªáu': cbf_thuong_hieu if cbf_thuong_hieu != "T·∫•t c·∫£" else "Honda",
                        'Gi√°': f"{cbf_gia} tri·ªáu",
                        'NƒÉm ƒëƒÉng k√Ω': cbf_nam,
                        'S·ªë Km ƒë√£ ƒëi': cbf_km,
                        'Ti√™u ƒë·ªÅ': f"Xe {cbf_thuong_hieu} {cbf_nam}"
                    }
                
                else:  # Ch·ªçn t·ª´ danh s√°ch
                    st.markdown("### Ch·ªçn xe ƒë·ªÉ t√¨m c√°c xe t∆∞∆°ng t·ª±")
                    
                    if 'Ti√™u ƒë·ªÅ' in sample_data.columns:
                        bike_options = sample_data['Ti√™u ƒë·ªÅ'].head(50).tolist()
                        selected_bike_title = st.selectbox("Ch·ªçn xe", bike_options, key="cbf_select_bike")
                        selected_bike = sample_data[sample_data['Ti√™u ƒë·ªÅ'] == selected_bike_title].iloc[0].to_dict()
                    else:
                        st.warning("Kh√¥ng c√≥ c·ªôt 'Ti√™u ƒë·ªÅ' trong d·ªØ li·ªáu")
                        selected_bike = None
                
                if selected_bike is not None:
                    top_n = st.slider("S·ªë xe t∆∞∆°ng t·ª±", min_value=1, max_value=20, value=5, key="cbf_top_n")
                    
                    if st.button("üîç T√¨m xe t∆∞∆°ng t·ª±", use_container_width=True, key="cbf_search"):
                        with st.spinner("ƒêang t√≠nh to√°n similarity..."):
                            # Prepare features for content-based
                            def prepare_content_features(df):
                                features = []
                                
                                # Helper functions
                                def safe_parse_year(value):
                                    if pd.isna(value):
                                        return 0
                                    try:
                                        import re
                                        value_str = str(value).strip()
                                        year_match = re.search(r'\d{4}', value_str)
                                        if year_match:
                                            year = int(year_match.group())
                                            if 1990 <= year <= 2025:
                                                return year
                                        return 0
                                    except:
                                        return 0
                                
                                def safe_parse_km(value):
                                    if pd.isna(value):
                                        return 0
                                    try:
                                        import re
                                        value_str = str(value).strip().lower()
                                        value_str = value_str.replace('km', '').replace(',', '').replace('.', '').strip()
                                        numbers = re.findall(r'\d+', value_str)
                                        if numbers:
                                            return float(''.join(numbers))
                                        return 0
                                    except:
                                        return 0
                                
                                for idx, row in df.iterrows():
                                    feature_vec = []
                                    
                                    # Brand (one-hot like)
                                    if 'Th∆∞∆°ng hi·ªáu' in row and pd.notna(row['Th∆∞∆°ng hi·ªáu']):
                                        brand = str(row['Th∆∞∆°ng hi·ªáu']).lower()
                                        feature_vec.append(hash(brand) % 1000 / 1000.0)
                                    else:
                                        feature_vec.append(0)
                                    
                                    # Price (normalized)
                                    from utils import parse_price
                                    price = parse_price(row.get('Gi√°', 0))
                                    if price and price > 0:
                                        feature_vec.append(price / 100.0)  # Normalize
                                    else:
                                        feature_vec.append(0)
                                    
                                    # Year (normalized) - handle string format
                                    year = safe_parse_year(row.get('NƒÉm ƒëƒÉng k√Ω', 0))
                                    if year > 0:
                                        feature_vec.append((year - 2000) / 25.0)  # Normalize
                                    else:
                                        feature_vec.append(0)
                                    
                                    # KM (normalized) - handle string format
                                    km = safe_parse_km(row.get('S·ªë Km ƒë√£ ƒëi', 0))
                                    if km > 0:
                                        feature_vec.append(km / 100000.0)  # Normalize
                                    else:
                                        feature_vec.append(0)
                                    
                                    features.append(feature_vec)
                                
                                return np.array(features)
                            
                            # Get features for all bikes
                            all_features = prepare_content_features(sample_data)
                            
                            # Calculate features for selected bike
                            if input_method == "üìù Nh·∫≠p th√¥ng tin tr·ª±c ti·∫øp":
                                # Create a temporary DataFrame with selected bike
                                temp_df = pd.DataFrame([selected_bike])
                                selected_features = prepare_content_features(temp_df)
                            else:
                                # Find index from selected bike title
                                if 'Ti√™u ƒë·ªÅ' in sample_data.columns and 'Ti√™u ƒë·ªÅ' in selected_bike:
                                    selected_bike_title = selected_bike.get('Ti√™u ƒë·ªÅ', '')
                                    matching = sample_data[sample_data['Ti√™u ƒë·ªÅ'] == selected_bike_title]
                                    if len(matching) > 0:
                                        selected_idx = matching.index[0]
                                        selected_features = all_features[selected_idx:selected_idx+1]
                                    else:
                                        st.error("Kh√¥ng t√¨m th·∫•y xe trong d·ªØ li·ªáu")
                                        selected_features = None
                                else:
                                    st.error("Kh√¥ng c√≥ th√¥ng tin 'Ti√™u ƒë·ªÅ'")
                                    selected_features = None
                            
                            if selected_features is not None and len(selected_features) > 0:
                                # Calculate cosine similarity
                                similarities = cosine_similarity(selected_features, all_features)[0]
                                
                                # Get top N similar
                                if input_method == "üìù Nh·∫≠p th√¥ng tin tr·ª±c ti·∫øp":
                                    # Don't exclude any (no "itself" when input directly)
                                    similar_indices = np.argsort(similarities)[::-1][:top_n]
                                else:
                                    # Exclude itself when selecting from list
                                    similar_indices = np.argsort(similarities)[::-1][1:top_n+1]
                            else:
                                st.error("Kh√¥ng th·ªÉ t√≠nh to√°n similarity")
                                similar_indices = []
                            
                            # Display results with better UX
                            st.success(f"‚úÖ T√¨m th·∫•y {len(similar_indices)} xe t∆∞∆°ng t·ª±")
                            
                            # Show selected bike info
                            if input_method == "üìù Nh·∫≠p th√¥ng tin tr·ª±c ti·∫øp":
                                st.info(f"üîç ƒêang t√¨m xe t∆∞∆°ng t·ª± v·ªõi: {selected_bike.get('Th∆∞∆°ng hi·ªáu', 'N/A')}, {selected_bike.get('Gi√°', 'N/A')}, nƒÉm {selected_bike.get('NƒÉm ƒëƒÉng k√Ω', 'N/A')}")
                            else:
                                st.info(f"üîç ƒêang t√¨m xe t∆∞∆°ng t·ª± v·ªõi: {selected_bike.get('Ti√™u ƒë·ªÅ', 'N/A')}")
                            
                            st.markdown("### üéØ K·∫øt qu·∫£ t√¨m ki·∫øm")
                            
                            for i, idx in enumerate(similar_indices, 1):
                                similar_bike = sample_data.iloc[idx]
                                similarity = similarities[idx]
                                
                                # Create a card-like display
                                with st.container():
                                    # Header with rank and similarity
                                    header_cols = st.columns([1, 4, 1])
                                    with header_cols[0]:
                                        st.markdown(f"### #{i}")
                                    with header_cols[1]:
                                        title = similar_bike.get('Ti√™u ƒë·ªÅ', 'N/A')
                                        st.markdown(f"**{title}**")
                                    with header_cols[2]:
                                        similarity_pct = similarity * 100
                                        st.metric("ƒê·ªô t∆∞∆°ng ƒë·ªìng", f"{similarity_pct:.1f}%")
                                    
                                    # Details in columns
                                    detail_cols = st.columns(4)
                                    with detail_cols[0]:
                                        from utils import format_price, parse_price
                                        price = parse_price(similar_bike.get('Gi√°', 0))
                                        st.metric("üí∞ Gi√°", format_price(price))
                                    with detail_cols[1]:
                                        brand = similar_bike.get('Th∆∞∆°ng hi·ªáu', 'N/A')
                                        st.metric("üèçÔ∏è Th∆∞∆°ng hi·ªáu", brand)
                                    with detail_cols[2]:
                                        year = similar_bike.get('NƒÉm ƒëƒÉng k√Ω', 'N/A')
                                        st.metric("üìÖ NƒÉm", str(year)[:4] if isinstance(year, (int, float)) else str(year)[:4] if len(str(year)) >= 4 else 'N/A')
                                    with detail_cols[3]:
                                        km = similar_bike.get('S·ªë Km ƒë√£ ƒëi', 'N/A')
                                        st.metric("üõ£Ô∏è S·ªë km", f"{km:,}" if isinstance(km, (int, float)) else str(km))
                                    
                                    # Similarity bar
                                    st.progress(similarity)
                                    st.caption(f"ƒê·ªô t∆∞∆°ng ƒë·ªìng: {similarity:.1%}")
                                    
                                    st.divider()
        
        with tab3:
            st.subheader("üìà Visualization - Tr·ª±c quan h√≥a ph√¢n kh√∫c")
            st.markdown("""
            **üí° Bi·ªÉu ƒë·ªì gi√∫p b·∫°n hi·ªÉu r√µ:**
            - üìä **C√≥ bao nhi√™u xe** trong m·ªói ph√¢n kh√∫c?
            - üí∞ **Gi√° trung b√¨nh** c·ªßa t·ª´ng ph√¢n kh√∫c l√† bao nhi√™u?
            - üìÖ **Xe ƒë·ªùi n√†o** ph·ªï bi·∫øn trong m·ªói ph√¢n kh√∫c?
            - üéØ **C√°c ph√¢n kh√∫c kh√°c nhau** nh∆∞ th·∫ø n√†o?
            
            > üí¨ **L∆∞u √Ω:** C·∫ßn ch·∫°y clustering ·ªü tab "Clustering" tr∆∞·ªõc ƒë·ªÉ xem visualization
            """)
            
            if 'cluster_labels' in st.session_state and 'cluster_data' in st.session_state:
                try:
                    cluster_labels = st.session_state['cluster_labels']
                    cluster_data = st.session_state['cluster_data']
                    X_vis = st.session_state.get('cluster_X', None)
                    
                    # Basic statistics with better explanations
                    st.markdown("### üìä 1. S·ªë l∆∞·ª£ng xe trong m·ªói ph√¢n kh√∫c")
                    st.markdown("Bi·ªÉu ƒë·ªì n√†y cho th·∫•y **c√≥ bao nhi√™u xe** trong m·ªói ph√¢n kh√∫c. Ph√¢n kh√∫c n√†o c√≥ nhi·ªÅu xe nh·∫•t?")
                    
                    cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()
                    cluster_counts_df = pd.DataFrame({
                        'Ph√¢n kh√∫c': [f'Ph√¢n kh√∫c {i}' for i in cluster_counts.index],
                        'S·ªë l∆∞·ª£ng xe': cluster_counts.values
                    })
                    
                    st.bar_chart(cluster_counts_df.set_index('Ph√¢n kh√∫c'))
                    
                    # Add explanation
                    max_cluster = cluster_counts.idxmax()
                    max_count = cluster_counts.max()
                    st.info(f"üí° **Ph√¢n kh√∫c {max_cluster}** c√≥ nhi·ªÅu xe nh·∫•t v·ªõi **{max_count} xe** ({max_count/len(cluster_data)*100:.1f}% t·ªïng s·ªë xe)")
                    
                    st.markdown("---")
                    
                    # Price distribution
                    st.markdown("### üí∞ 2. Gi√° trung b√¨nh c·ªßa t·ª´ng ph√¢n kh√∫c")
                    st.markdown("Bi·ªÉu ƒë·ªì n√†y cho th·∫•y **gi√° trung b√¨nh** c·ªßa m·ªói ph√¢n kh√∫c. Ph√¢n kh√∫c n√†o ƒë·∫Øt nh·∫•t? R·∫ª nh·∫•t?")
                    
                    col1, col2 = st.columns([2, 1])
                    
                    with col1:
                        # Price distribution by cluster
                        if 'price_parsed' in cluster_data.columns:
                            price_by_cluster = cluster_data.groupby('cluster')['price_parsed'].mean().sort_index()
                        elif 'Gi√°' in cluster_data.columns:
                            from utils import parse_price
                            cluster_data['price_temp'] = cluster_data['Gi√°'].apply(parse_price)
                            price_by_cluster = cluster_data.groupby('cluster')['price_temp'].mean().sort_index()
                        else:
                            price_by_cluster = pd.Series()
                        
                        if len(price_by_cluster) > 0:
                            price_df = pd.DataFrame({
                                'Ph√¢n kh√∫c': [f'Ph√¢n kh√∫c {i}' for i in price_by_cluster.index],
                                'Gi√° trung b√¨nh (tri·ªáu VNƒê)': price_by_cluster.values
                            })
                            st.bar_chart(price_df.set_index('Ph√¢n kh√∫c'))
                            
                            # Add explanation
                            cheapest = price_by_cluster.idxmin()
                            most_expensive = price_by_cluster.idxmax()
                            with col2:
                                st.metric("üí∞ R·∫ª nh·∫•t", f"Ph√¢n kh√∫c {cheapest}", f"{price_by_cluster[cheapest]:.1f} tri·ªáu")
                                st.metric("üíé ƒê·∫Øt nh·∫•t", f"Ph√¢n kh√∫c {most_expensive}", f"{price_by_cluster[most_expensive]:.1f} tri·ªáu")
                                st.caption(f"Ch√™nh l·ªách: {price_by_cluster[most_expensive] - price_by_cluster[cheapest]:.1f} tri·ªáu")
                    
                    st.markdown("---")
                    
                    # Year distribution
                    st.markdown("### üìÖ 3. NƒÉm s·∫£n xu·∫•t trung b√¨nh c·ªßa t·ª´ng ph√¢n kh√∫c")
                    st.markdown("Bi·ªÉu ƒë·ªì n√†y cho th·∫•y **xe ƒë·ªùi n√†o** ph·ªï bi·∫øn trong m·ªói ph√¢n kh√∫c. Ph√¢n kh√∫c n√†o c√≥ xe m·ªõi nh·∫•t?")
                    
                    if 'year_parsed' in cluster_data.columns:
                        year_by_cluster = cluster_data.groupby('cluster')['year_parsed'].mean().sort_index()
                        year_df = pd.DataFrame({
                            'Ph√¢n kh√∫c': [f'Ph√¢n kh√∫c {i}' for i in year_by_cluster.index],
                            'NƒÉm trung b√¨nh': year_by_cluster.values
                        })
                        st.bar_chart(year_df.set_index('Ph√¢n kh√∫c'))
                        
                        # Add explanation
                        newest = year_by_cluster.idxmax()
                        oldest = year_by_cluster.idxmin()
                        col1, col2 = st.columns(2)
                        with col1:
                            st.info(f"üÜï **Ph√¢n kh√∫c {newest}** c√≥ xe m·ªõi nh·∫•t (nƒÉm TB: {year_by_cluster[newest]:.0f})")
                        with col2:
                            st.info(f"üìú **Ph√¢n kh√∫c {oldest}** c√≥ xe c≈© nh·∫•t (nƒÉm TB: {year_by_cluster[oldest]:.0f})")
                    
                    st.markdown("---")
                    
                    # Brand distribution
                    if 'Th∆∞∆°ng hi·ªáu' in cluster_data.columns:
                        st.markdown("### üèçÔ∏è Th∆∞∆°ng hi·ªáu ph·ªï bi·∫øn theo c·ª•m")
                        for cluster_id in sorted(cluster_data['cluster'].unique()):
                            cluster_bikes = cluster_data[cluster_data['cluster'] == cluster_id]
                            if len(cluster_bikes) > 0:
                                brand_counts = cluster_bikes['Th∆∞∆°ng hi·ªáu'].value_counts().head(5)
                                if len(brand_counts) > 0:
                                    st.write(f"**C·ª•m {cluster_id}:** {', '.join(brand_counts.index.tolist())}")
                    
                    # 2D visualization if we have features
                    if X_vis is not None and X_vis.shape[1] >= 2:
                        st.markdown("### üéØ 5. B·∫£n ƒë·ªì ph√¢n kh√∫c (Bi·ªÉu ƒë·ªì 2D)")
                        st.markdown("""
                        **Bi·ªÉu ƒë·ªì n√†y gi√∫p b·∫°n hi·ªÉu:**
                        - üéØ **V·ªã tr√≠** c·ªßa t·ª´ng ph√¢n kh√∫c trong kh√¥ng gian 2 chi·ªÅu
                        - üìç **Kho·∫£ng c√°ch** gi·ªØa c√°c ph√¢n kh√∫c (ph√¢n kh√∫c g·∫ßn nhau = t∆∞∆°ng ƒë·ªìng)
                        - üîç **M·∫≠t ƒë·ªô** xe trong m·ªói ph√¢n kh√∫c (ƒëi·ªÉm d√†y = nhi·ªÅu xe)
                        
                        > üí° **C√°ch ƒë·ªçc:** M·ªói ch·∫•m l√† m·ªôt xe. C√°c ch·∫•m c√πng m√†u = c√πng ph√¢n kh√∫c. Ch·∫•m g·∫ßn nhau = ƒë·∫∑c ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng.
                        """)
                        
                        try:
                            from sklearn.decomposition import PCA
                            
                            # Reduce to 2D
                            pca = PCA(n_components=2, random_state=42)
                            X_2d = pca.fit_transform(X_vis)
                            
                            # Create plot with better styling
                            fig, ax = plt.subplots(figsize=(14, 10))
                            
                            # Use distinct colors for each cluster
                            colors_list = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8', '#F7DC6F', '#BB8FCE', '#85C1E2']
                            
                            for cluster_id in np.unique(cluster_labels):
                                mask = cluster_labels == cluster_id
                                color = colors_list[cluster_id % len(colors_list)]
                                ax.scatter(
                                    X_2d[mask, 0], X_2d[mask, 1],
                                    c=color,
                                    label=f'Ph√¢n kh√∫c {cluster_id} ({np.sum(mask)} xe)',
                                    alpha=0.7,
                                    s=80,
                                    edgecolors='white',
                                    linewidth=0.5
                                )
                            
                            # Better labels in Vietnamese
                            variance_pc1 = pca.explained_variance_ratio_[0] * 100
                            variance_pc2 = pca.explained_variance_ratio_[1] * 100
                            
                            ax.set_xlabel(f'Tr·ª•c 1 - Gi·∫£i th√≠ch {variance_pc1:.1f}% s·ª± kh√°c bi·ªát', fontsize=12, fontweight='bold')
                            ax.set_ylabel(f'Tr·ª•c 2 - Gi·∫£i th√≠ch {variance_pc2:.1f}% s·ª± kh√°c bi·ªát', fontsize=12, fontweight='bold')
                            ax.set_title('üó∫Ô∏è B·∫£n ƒë·ªì c√°c ph√¢n kh√∫c xe m√°y', fontsize=14, fontweight='bold', pad=20)
                            ax.legend(title='üìä Ph√¢n kh√∫c', title_fontsize=12, fontsize=10, 
                                    bbox_to_anchor=(1.02, 1), loc='upper left', frameon=True, fancybox=True, shadow=True)
                            ax.grid(True, alpha=0.3, linestyle='--')
                            ax.set_facecolor('#f8f9fa')
                            plt.tight_layout()
                            st.pyplot(fig)
                            plt.close(fig)
                            
                            # Add explanation
                            st.success(f"""
                            ‚úÖ **Bi·ªÉu ƒë·ªì ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!**
                            
                            **C√°ch hi·ªÉu bi·ªÉu ƒë·ªì:**
                            - M·ªói ch·∫•m m√†u = m·ªôt xe m√°y
                            - Ch·∫•m c√πng m√†u = c√πng ph√¢n kh√∫c
                            - Ch·∫•m g·∫ßn nhau = ƒë·∫∑c ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng (gi√°, nƒÉm, th∆∞∆°ng hi·ªáu...)
                            - Ch·∫•m xa nhau = kh√°c bi·ªát nhi·ªÅu
                            
                            **V√≠ d·ª•:** N·∫øu ph√¢n kh√∫c 0 v√† ph√¢n kh√∫c 1 g·∫ßn nhau ‚Üí hai ph√¢n kh√∫c n√†y c√≥ ƒë·∫∑c ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng, c√≥ th·ªÉ g·ªôp l·∫°i ho·∫∑c c·∫ßn ph√¢n bi·ªát r√µ h∆°n.
                            """)
                        except Exception as e:
                            st.warning(f"Kh√¥ng th·ªÉ t·∫°o bi·ªÉu ƒë·ªì 2D: {str(e)}")
                            import traceback
                            with st.expander("Chi ti·∫øt l·ªói"):
                                st.code(traceback.format_exc())
                    
                    # Summary insights - User-friendly
                    st.markdown("---")
                    st.subheader("üí° T√≥m t·∫Øt - Nh·ªØng ƒëi·ªÅu quan tr·ªçng c·∫ßn bi·∫øt")
                    st.markdown("D·ª±a tr√™n k·∫øt qu·∫£ ph√¢n c·ª•m, ƒë√¢y l√† nh·ªØng **insights ch√≠nh** gi√∫p b·∫°n hi·ªÉu th·ªã tr∆∞·ªùng:")
                    
                    # Price analysis
                    if 'price_parsed' in cluster_data.columns or 'Gi√°' in cluster_data.columns:
                        all_prices = []
                        for cluster_id in range(n_clusters):
                            cluster_subset = cluster_data[cluster_data['cluster'] == cluster_id]
                            if 'price_parsed' in cluster_subset.columns:
                                prices = cluster_subset['price_parsed'].dropna()
                            elif 'Gi√°' in cluster_subset.columns:
                                from utils import parse_price
                                prices = cluster_subset['Gi√°'].apply(parse_price).dropna()
                            else:
                                prices = pd.Series()
                            if len(prices) > 0:
                                all_prices.append((cluster_id, prices.mean(), prices.min(), prices.max(), len(cluster_subset)))
                        
                        if all_prices:
                            all_prices.sort(key=lambda x: x[1])
                            cheapest = all_prices[0]
                            most_expensive = all_prices[-1]
                            
                            # Display in cards
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                st.markdown(f"""
                                #### üí∞ Ph√¢n kh√∫c gi√° r·∫ª nh·∫•t
                                **Ph√¢n kh√∫c {cheapest[0]}**
                                - Gi√° trung b√¨nh: **{cheapest[1]:.1f} tri·ªáu VNƒê**
                                - Kho·∫£ng gi√°: {cheapest[2]:.1f} - {cheapest[3]:.1f} tri·ªáu
                                - S·ªë l∆∞·ª£ng: {cheapest[4]} xe
                                
                                üí° **Ph√π h·ª£p cho:** Ng∆∞·ªùi c√≥ ng√¢n s√°ch h·∫°n ch·∫ø, sinh vi√™n, ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu
                                """)
                            
                            with col2:
                                st.markdown(f"""
                                #### üíé Ph√¢n kh√∫c gi√° cao nh·∫•t
                                **Ph√¢n kh√∫c {most_expensive[0]}**
                                - Gi√° trung b√¨nh: **{most_expensive[1]:.1f} tri·ªáu VNƒê**
                                - Kho·∫£ng gi√°: {most_expensive[2]:.1f} - {most_expensive[3]:.1f} tri·ªáu
                                - S·ªë l∆∞·ª£ng: {most_expensive[4]} xe
                                
                                üí° **Ph√π h·ª£p cho:** Ng∆∞·ªùi c√≥ thu nh·∫≠p cao, mu·ªën xe cao c·∫•p, ƒë·ªùi m·ªõi
                                """)
                            
                            # Price difference
                            price_diff = most_expensive[1] - cheapest[1]
                            st.info(f"üìä **Ch√™nh l·ªách gi√°:** Ph√¢n kh√∫c ƒë·∫Øt nh·∫•t cao h∆°n ph√¢n kh√∫c r·∫ª nh·∫•t **{price_diff:.1f} tri·ªáu VNƒê** ({price_diff/cheapest[1]*100:.0f}%)")
                    
                    # Market share
                    st.markdown("### üìä Th·ªã ph·∫ßn c√°c ph√¢n kh√∫c")
                    cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()
                    for cluster_id, count in cluster_counts.items():
                        percentage = count / len(cluster_data) * 100
                        st.progress(percentage / 100, text=f"Ph√¢n kh√∫c {cluster_id}: {count} xe ({percentage:.1f}% th·ªã tr∆∞·ªùng)")
                    
                    # Final message
                    st.success("""
                    ‚úÖ **Ph√¢n c·ª•m ho√†n t·∫•t!**
                    
                    **B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng k·∫øt qu·∫£ n√†y ƒë·ªÉ:**
                    - üõí **Ng∆∞·ªùi mua:** T√¨m ph√¢n kh√∫c ph√π h·ª£p v·ªõi ng√¢n s√°ch
                    - üíº **Ng∆∞·ªùi b√°n:** ƒê·ªãnh gi√° h·ª£p l√Ω d·ª±a tr√™n ph√¢n kh√∫c
                    - üìà **Ph√¢n t√≠ch:** Hi·ªÉu c·∫•u tr√∫c v√† xu h∆∞·ªõng th·ªã tr∆∞·ªùng
                    """)
                    
                except Exception as e:
                    st.error(f"L·ªói khi hi·ªÉn th·ªã visualization: {str(e)}")
                    import traceback
                    st.code(traceback.format_exc())
            else:
                st.info("üí° Ch·∫°y clustering ·ªü tab 'Clustering' tr∆∞·ªõc ƒë·ªÉ xem visualization")
                st.markdown("""
                ### C√°c t√≠nh nƒÉng visualization s·∫Ω c√≥:
                - Bi·ªÉu ƒë·ªì s·ªë l∆∞·ª£ng xe trong m·ªói c·ª•m
                - Ph√¢n b·ªë gi√° theo c·ª•m
                - Ph√¢n b·ªë nƒÉm theo c·ª•m
                - Bi·ªÉu ƒë·ªì 2D/3D v·ªõi PCA
                """)

# Footer
st.sidebar.markdown("---")
st.sidebar.markdown("### üë§ Th√¥ng tin")
st.sidebar.markdown("""
**üë®‚Äçüíª T√°c gi·∫£:** ƒêo√†n Anh  
**üéì ƒê·ªì √°n:** Data Science  
**üìä Dataset:** Ch·ª£ T·ªët  
**üìà S·ªë l∆∞·ª£ng:** 7,200+ records
""")
st.sidebar.markdown("### üìö T√†i li·ªáu")
st.sidebar.markdown("[GitHub Repository](https://github.com/teddyDn2001/ProjectChoTot)")
st.sidebar.markdown("[README](README.md)")

