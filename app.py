"""
Streamlit App - Motorbike Analysis Platform
·ª®ng d·ª•ng web t√≠ch h·ª£p c√°c t√≠nh nƒÉng t·ª´ Project 1 v√† Project 2
"""
import streamlit as st
import pandas as pd
import numpy as np
from pathlib import Path
import sys

# Add project paths to sys.path
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT / "project1"))
sys.path.insert(0, str(PROJECT_ROOT / "project2"))

# Page config
st.set_page_config(
    page_title="Motorbike Analysis Platform",
    page_icon="üèçÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    .stButton>button {
        width: 100%;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state
if 'data_loaded' not in st.session_state:
    st.session_state.data_loaded = False
if 'models_loaded' not in st.session_state:
    st.session_state.models_loaded = False

# Sidebar navigation
st.sidebar.title("üèçÔ∏è Motorbike Analysis")
st.sidebar.markdown("---")

page = st.sidebar.radio(
    "Ch·ªçn ch·ª©c nƒÉng:",
    ["üè† Trang ch·ªß", "üí∞ D·ª± ƒëo√°n gi√°", "üö® Ph√°t hi·ªán b·∫•t th∆∞·ªùng", "üîç G·ª£i √Ω xe t∆∞∆°ng t·ª±", "üìä Ph√¢n c·ª•m d·ªØ li·ªáu"]
)

# Import modules (lazy loading)
@st.cache_resource
def load_price_model():
    """Load price prediction model"""
    try:
        from project1.config import PRICE_MODEL_PATH, PREPROCESSOR_PATH
        import joblib
        
        if not PRICE_MODEL_PATH.exists() or not PREPROCESSOR_PATH.exists():
            return None, None, "Models ch∆∞a ƒë∆∞·ª£c train. Vui l√≤ng ch·∫°y notebooks trong project1/ ƒë·ªÉ t·∫°o models."
        
        model = joblib.load(PRICE_MODEL_PATH)
        preprocessor_data = joblib.load(PREPROCESSOR_PATH)
        preprocessor = preprocessor_data['preprocessor']
        return model, preprocessor, None
    except Exception as e:
        return None, None, f"L·ªói khi load model: {str(e)}"

@st.cache_resource
def load_anomaly_model():
    """Load anomaly detection model"""
    try:
        from project1.config import ISO_MODEL_PATH, PREPROCESSOR_PATH
        import joblib
        
        if not ISO_MODEL_PATH.exists() or not PREPROCESSOR_PATH.exists():
            return None, None, "Models ch∆∞a ƒë∆∞·ª£c train."
        
        iso_data = joblib.load(ISO_MODEL_PATH)
        # Check if it's a dict (saved with metadata) or direct model
        if isinstance(iso_data, dict):
            iso_model = iso_data.get('model', iso_data.get('iso_model', iso_data))
        else:
            iso_model = iso_data
        
        preprocessor_data = joblib.load(PREPROCESSOR_PATH)
        preprocessor = preprocessor_data['preprocessor']
        return iso_model, preprocessor, None
    except Exception as e:
        return None, None, f"L·ªói khi load model: {str(e)}"

@st.cache_data
def load_sample_data():
    """Load sample data for recommendation"""
    try:
        from project2.config import RAW_DATA_FILE, DATA_DIR
        
        # Try multiple paths
        possible_paths = [
            RAW_DATA_FILE,  # data/data_motobikes.xlsx - Sheet1.csv
            PROJECT_ROOT / "data" / "data_motobikes.xlsx - Sheet1.csv",
            PROJECT_ROOT / "project2" / "data_motobikes.xlsx - Sheet1.csv",
            PROJECT_ROOT / "project1" / "data_motobikes.xlsx - Sheet1.csv",
        ]
        
        for path in possible_paths:
            if path.exists():
                df = pd.read_csv(path, nrows=1000, low_memory=False)  # Load sample
                return df, None
        
        return None, f"Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu. ƒê√£ th·ª≠: {[str(p) for p in possible_paths]}"
    except Exception as e:
        return None, f"L·ªói khi load d·ªØ li·ªáu: {str(e)}"

# Home page
if page == "üè† Trang ch·ªß":
    st.markdown('<div class="main-header">üèçÔ∏è Motorbike Analysis Platform</div>', unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("### üí∞ D·ª± ƒëo√°n gi√°")
        st.markdown("""
        D·ª± ƒëo√°n gi√° xe m√°y d·ª±a tr√™n:
        - Th∆∞∆°ng hi·ªáu, d√≤ng xe
        - NƒÉm ƒëƒÉng k√Ω, s·ªë km
        - T√¨nh tr·∫°ng, dung t√≠ch
        """)
    
    with col2:
        st.markdown("### üö® Ph√°t hi·ªán b·∫•t th∆∞·ªùng")
        st.markdown("""
        Ph√°t hi·ªán c√°c tin ƒëƒÉng c√≥ gi√° b·∫•t th∆∞·ªùng:
        - Residual-based detection
        - Isolation Forest
        """)
    
    with col3:
        st.markdown("### üîç G·ª£i √Ω")
        st.markdown("""
        T√¨m xe m√°y t∆∞∆°ng t·ª±:
        - KNN-based recommendation
        - Content-based filtering
        """)
    
    st.markdown("---")
    st.markdown("### üìä Th·ªëng k√™")
    
    # Check models status
    price_model, _, price_err = load_price_model()
    anomaly_model, _, anomaly_err = load_anomaly_model()
    sample_data, data_err = load_sample_data()
    
    status_col1, status_col2, status_col3 = st.columns(3)
    
    with status_col1:
        if price_model:
            st.success("‚úÖ Price Model: S·∫µn s√†ng")
        else:
            st.error(f"‚ùå Price Model: {price_err or 'Ch∆∞a load'}")
    
    with status_col2:
        if anomaly_model:
            st.success("‚úÖ Anomaly Model: S·∫µn s√†ng")
        else:
            st.error(f"‚ùå Anomaly Model: {anomaly_err or 'Ch∆∞a load'}")
    
    with status_col3:
        if sample_data is not None:
            st.success(f"‚úÖ Data: {len(sample_data)} records")
        else:
            st.error(f"‚ùå Data: {data_err or 'Ch∆∞a load'}")

# Price Prediction page
elif page == "üí∞ D·ª± ƒëo√°n gi√°":
    st.title("üí∞ D·ª± ƒëo√°n gi√° xe m√°y")
    st.markdown("Nh·∫≠p th√¥ng tin xe ƒë·ªÉ d·ª± ƒëo√°n gi√°")
    
    model, preprocessor, error = load_price_model()
    
    if error:
        st.error(error)
        st.info("üí° H∆∞·ªõng d·∫´n: Ch·∫°y c√°c notebooks trong project1/ ƒë·ªÉ train models tr∆∞·ªõc.")
    else:
        with st.form("price_prediction_form"):
            col1, col2 = st.columns(2)
            
            with col1:
                thuong_hieu = st.selectbox("Th∆∞∆°ng hi·ªáu", ["Honda", "Yamaha", "SYM", "Piaggio", "Vespa", "Kh√°c"])
                dong_xe = st.text_input("D√≤ng xe", placeholder="V√≠ d·ª•: SH, Air Blade, Exciter")
                nam_dang_ky = st.number_input("NƒÉm ƒëƒÉng k√Ω", min_value=1990, max_value=2024, value=2020)
                so_km = st.number_input("S·ªë km ƒë√£ ƒëi", min_value=0, value=10000)
            
            with col2:
                tinh_trang = st.selectbox("T√¨nh tr·∫°ng", ["M·ªõi", "ƒê√£ s·ª≠ d·ª•ng", "C·∫ßn s·ª≠a ch·ªØa"])
                loai_xe = st.selectbox("Lo·∫°i xe", ["Tay ga", "S·ªë", "Tay c√¥n", "Kh√°c"])
                xuat_xu = st.selectbox("Xu·∫•t x·ª©", ["Vi·ªát Nam", "Th√°i Lan", "Indonesia", "Nh·∫≠t B·∫£n", "Kh√°c"])
                dung_tich_cc = st.number_input("Dung t√≠ch (cc)", min_value=50, max_value=1000, value=125)
            
            tinh_thanh = st.selectbox("T·ªânh/Th√†nh", ["H·ªì Ch√≠ Minh", "H√† N·ªôi", "ƒê√† N·∫µng", "Kh√°c"])
            quan = st.text_input("Qu·∫≠n/Huy·ªán", placeholder="V√≠ d·ª•: Qu·∫≠n 1, Qu·∫≠n 7")
            
            submitted = st.form_submit_button("üîÆ D·ª± ƒëo√°n gi√°", use_container_width=True)
            
            if submitted:
                try:
                    # Prepare input data
                    input_data = pd.DataFrame({
                        'so_km': [so_km],
                        'nam_dang_ky': [nam_dang_ky],
                        'dung_tich_cc': [dung_tich_cc],
                        'trong_luong_kg': [np.nan],  # Will be imputed
                        'len_title': [len(dong_xe)],
                        'len_desc': [0],
                        'thuong_hieu': [thuong_hieu],
                        'dong_xe': [dong_xe],
                        'tinh_trang': [tinh_trang],
                        'loai_xe': [loai_xe],
                        'xuat_xu': [xuat_xu],
                        'tinh_thanh': [tinh_thanh],
                        'quan': [quan]
                    })
                    
                    # Transform and predict
                    X_transformed = preprocessor.transform(input_data)
                    prediction = model.predict(X_transformed)[0]
                    
                    # Display result
                    st.success(f"### üí∞ Gi√° d·ª± ƒëo√°n: {prediction:,.0f} VNƒê")
                    st.info(f"‚âà {prediction/1_000_000:.2f} tri·ªáu VNƒê")
                    
                except Exception as e:
                    st.error(f"L·ªói khi d·ª± ƒëo√°n: {str(e)}")

# Anomaly Detection page
elif page == "üö® Ph√°t hi·ªán b·∫•t th∆∞·ªùng":
    st.title("üö® Ph√°t hi·ªán gi√° b·∫•t th∆∞·ªùng")
    st.markdown("Ki·ªÉm tra xem gi√° xe c√≥ b·∫•t th∆∞·ªùng so v·ªõi th·ªã tr∆∞·ªùng kh√¥ng")
    
    model, preprocessor, error = load_anomaly_model()
    
    if error:
        st.error(error)
    else:
        st.info("Nh·∫≠p th√¥ng tin xe v√† gi√° ƒë·ªÉ ki·ªÉm tra")
        
        with st.form("anomaly_detection_form"):
            col1, col2 = st.columns(2)
            
            with col1:
                thuong_hieu = st.selectbox("Th∆∞∆°ng hi·ªáu", ["Honda", "Yamaha", "SYM", "Piaggio", "Vespa"])
                dong_xe = st.text_input("D√≤ng xe")
                nam_dang_ky = st.number_input("NƒÉm ƒëƒÉng k√Ω", min_value=1990, max_value=2024)
                so_km = st.number_input("S·ªë km", min_value=0)
            
            with col2:
                tinh_trang = st.selectbox("T√¨nh tr·∫°ng", ["M·ªõi", "ƒê√£ s·ª≠ d·ª•ng", "C·∫ßn s·ª≠a ch·ªØa"])
                loai_xe = st.selectbox("Lo·∫°i xe", ["Tay ga", "S·ªë", "Tay c√¥n"])
                dung_tich_cc = st.number_input("Dung t√≠ch (cc)", min_value=50, max_value=1000)
                gia_vnd = st.number_input("Gi√° (VNƒê)", min_value=0, format="%d")
            
            submitted = st.form_submit_button("üîç Ki·ªÉm tra", use_container_width=True)
            
            if submitted:
                try:
                    # Prepare input
                    input_data = pd.DataFrame({
                        'so_km': [so_km],
                        'nam_dang_ky': [nam_dang_ky],
                        'dung_tich_cc': [dung_tich_cc],
                        'trong_luong_kg': [np.nan],
                        'len_title': [len(dong_xe)],
                        'len_desc': [0],
                        'thuong_hieu': [thuong_hieu],
                        'dong_xe': [dong_xe],
                        'tinh_trang': [tinh_trang],
                        'loai_xe': [loai_xe],
                        'xuat_xu': ["Vi·ªát Nam"],
                        'tinh_thanh': ["H·ªì Ch√≠ Minh"],
                        'quan': [""]
                    })
                    
                    # Transform
                    X_transformed = preprocessor.transform(input_data)
                    
                    # Predict anomaly
                    anomaly_score = model.decision_function(X_transformed)[0]
                    is_anomaly = model.predict(X_transformed)[0] == -1
                    
                    # Display result
                    if is_anomaly:
                        st.error("### ‚ö†Ô∏è Ph√°t hi·ªán gi√° B·∫§T TH∆Ø·ªúNG")
                        st.warning(f"Anomaly score: {anomaly_score:.4f}")
                        st.info("Gi√° n√†y c√≥ v·∫ª kh√¥ng ph√π h·ª£p v·ªõi th·ªã tr∆∞·ªùng. N√™n ki·ªÉm tra l·∫°i.")
                    else:
                        st.success("### ‚úÖ Gi√° B√åNH TH∆Ø·ªúNG")
                        st.info(f"Anomaly score: {anomaly_score:.4f}")
                        
                except Exception as e:
                    st.error(f"L·ªói: {str(e)}")

# Recommendation page
elif page == "üîç G·ª£i √Ω xe t∆∞∆°ng t·ª±":
    st.title("üîç T√¨m xe m√°y t∆∞∆°ng t·ª±")
    st.markdown("Nh·∫≠p ID ho·∫∑c th√¥ng tin xe ƒë·ªÉ t√¨m c√°c xe t∆∞∆°ng t·ª±")
    
    sample_data, error = load_sample_data()
    
    if error:
        st.error(error)
    else:
        st.info(f"üìä ƒêang load {len(sample_data)} records t·ª´ d·ªØ li·ªáu")
        
        # Import utils
        from utils import get_bike_info, find_similar_bikes, format_price, parse_price
        
        # Simple recommendation interface
        st.subheader("üîé T√¨m ki·∫øm")
        search_option = st.radio("T√¨m theo:", ["ID", "Th∆∞∆°ng hi·ªáu", "D√≤ng xe", "Th√¥ng tin t√πy ch·ªânh"], horizontal=True)
        
        if search_option == "ID":
            col1, col2 = st.columns([3, 1])
            with col1:
                bike_id = st.text_input("Nh·∫≠p ID xe", placeholder="V√≠ d·ª•: 12345")
            with col2:
                top_n = st.number_input("S·ªë k·∫øt qu·∫£", min_value=1, max_value=20, value=5)
            
            if bike_id and st.button("üîç T√¨m xe t∆∞∆°ng t·ª±", use_container_width=True):
                bike_info = get_bike_info(sample_data, bike_id)
                if bike_info:
                    st.success(f"‚úÖ T√¨m th·∫•y xe: {bike_info.get('Ti√™u ƒë·ªÅ', bike_info.get('tieu_de', 'N/A'))}")
                    
                    # Show bike info
                    with st.expander("üìã Th√¥ng tin xe", expanded=False):
                        info_cols = st.columns(3)
                        with info_cols[0]:
                            st.metric("Th∆∞∆°ng hi·ªáu", bike_info.get('Th∆∞∆°ng hi·ªáu', bike_info.get('thuong_hieu', 'N/A')))
                        with info_cols[1]:
                            price = parse_price(bike_info.get('Gi√°', bike_info.get('gia_vnd', None)))
                            st.metric("Gi√°", format_price(price))
                        with info_cols[2]:
                            st.metric("NƒÉm", bike_info.get('NƒÉm ƒëƒÉng k√Ω', bike_info.get('nam_dang_ky', 'N/A')))
                    
                    # Find similar
                    similar = find_similar_bikes(bike_info, sample_data, top_n=top_n)
                    
                    if similar:
                        st.subheader(f"üéØ {len(similar)} xe t∆∞∆°ng t·ª±")
                        for i, bike in enumerate(similar, 1):
                            with st.container():
                                cols = st.columns([1, 2, 1, 1])
                                with cols[0]:
                                    st.write(f"**#{i}**")
                                with cols[1]:
                                    title = bike.get('Ti√™u ƒë·ªÅ', bike.get('tieu_de', 'N/A'))
                                    st.write(f"**{title}**")
                                with cols[2]:
                                    price = parse_price(bike.get('Gi√°', bike.get('gia_vnd', None)))
                                    st.write(format_price(price))
                                with cols[3]:
                                    st.write(bike.get('Th∆∞∆°ng hi·ªáu', bike.get('thuong_hieu', 'N/A')))
                                st.divider()
                    else:
                        st.warning("Kh√¥ng t√¨m th·∫•y xe t∆∞∆°ng t·ª±")
                else:
                    st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y xe v·ªõi ID: {bike_id}")
                    st.info("üí° Th·ª≠ t√¨m theo th∆∞∆°ng hi·ªáu ho·∫∑c d√≤ng xe")
        
        elif search_option == "Th∆∞∆°ng hi·ªáu":
            brands = sorted(sample_data['Th∆∞∆°ng hi·ªáu'].dropna().unique()) if 'Th∆∞∆°ng hi·ªáu' in sample_data.columns else []
            if brands:
                selected_brand = st.selectbox("Ch·ªçn th∆∞∆°ng hi·ªáu", brands)
                if st.button("üîç T√¨m", use_container_width=True):
                    filtered = sample_data[sample_data['Th∆∞∆°ng hi·ªáu'] == selected_brand]
                    st.subheader(f"üìä T√¨m th·∫•y {len(filtered)} xe {selected_brand}")
                    st.dataframe(
                        filtered[['Ti√™u ƒë·ªÅ', 'Gi√°', 'NƒÉm ƒëƒÉng k√Ω', 'S·ªë Km ƒë√£ ƒëi']].head(20),
                        use_container_width=True,
                        hide_index=True
                    )
            else:
                st.warning("Kh√¥ng c√≥ c·ªôt 'Th∆∞∆°ng hi·ªáu' trong d·ªØ li·ªáu")
        
        elif search_option == "D√≤ng xe":
            model_name = st.text_input("Nh·∫≠p t√™n d√≤ng xe", placeholder="V√≠ d·ª•: SH, Air Blade")
            if model_name and st.button("üîç T√¨m", use_container_width=True):
                if 'D√≤ng xe' in sample_data.columns:
                    filtered = sample_data[sample_data['D√≤ng xe'].str.contains(model_name, case=False, na=False)]
                    st.subheader(f"üìä T√¨m th·∫•y {len(filtered)} xe")
                    st.dataframe(
                        filtered[['Ti√™u ƒë·ªÅ', 'Gi√°', 'Th∆∞∆°ng hi·ªáu', 'NƒÉm ƒëƒÉng k√Ω']].head(20),
                        use_container_width=True,
                        hide_index=True
                    )
                else:
                    st.warning("Kh√¥ng c√≥ c·ªôt 'D√≤ng xe' trong d·ªØ li·ªáu")
        
        elif search_option == "Th√¥ng tin t√πy ch·ªânh":
            st.subheader("üîß T√¨m ki·∫øm n√¢ng cao")
            col1, col2 = st.columns(2)
            with col1:
                brand = st.selectbox("Th∆∞∆°ng hi·ªáu", ["T·∫•t c·∫£"] + sorted(sample_data['Th∆∞∆°ng hi·ªáu'].dropna().unique().tolist()) if 'Th∆∞∆°ng hi·ªáu' in sample_data.columns else ["T·∫•t c·∫£"])
                min_price = st.number_input("Gi√° t·ªëi thi·ªÉu (tri·ªáu)", min_value=0, value=0)
                max_price = st.number_input("Gi√° t·ªëi ƒëa (tri·ªáu)", min_value=0, value=500)
            with col2:
                min_year = st.number_input("NƒÉm t·ªëi thi·ªÉu", min_value=1990, max_value=2024, value=2010)
                max_year = st.number_input("NƒÉm t·ªëi ƒëa", min_value=1990, max_value=2024, value=2024)
            
            if st.button("üîç T√¨m ki·∫øm", use_container_width=True):
                filtered = sample_data.copy()
                
                # Filter by brand
                if brand != "T·∫•t c·∫£" and 'Th∆∞∆°ng hi·ªáu' in filtered.columns:
                    filtered = filtered[filtered['Th∆∞∆°ng hi·ªáu'] == brand]
                
                # Filter by price
                if 'Gi√°' in filtered.columns:
                    from utils import parse_price
                    filtered['price_parsed'] = filtered['Gi√°'].apply(parse_price)
                    filtered = filtered[(filtered['price_parsed'] >= min_price) & (filtered['price_parsed'] <= max_price)]
                
                # Filter by year
                if 'NƒÉm ƒëƒÉng k√Ω' in filtered.columns:
                    filtered = filtered[(filtered['NƒÉm ƒëƒÉng k√Ω'] >= min_year) & (filtered['NƒÉm ƒëƒÉng k√Ω'] <= max_year)]
                
                st.subheader(f"üìä T√¨m th·∫•y {len(filtered)} xe ph√π h·ª£p")
                if len(filtered) > 0:
                    display_cols = ['Ti√™u ƒë·ªÅ', 'Gi√°', 'Th∆∞∆°ng hi·ªáu', 'NƒÉm ƒëƒÉng k√Ω']
                    available_cols = [col for col in display_cols if col in filtered.columns]
                    st.dataframe(
                        filtered[available_cols].head(50),
                        use_container_width=True,
                        hide_index=True
                    )
                else:
                    st.info("Kh√¥ng t√¨m th·∫•y xe ph√π h·ª£p v·ªõi ti√™u ch√≠")

# Clustering page
elif page == "üìä Ph√¢n c·ª•m d·ªØ li·ªáu":
    st.title("üìä Ph√¢n c·ª•m d·ªØ li·ªáu")
    st.markdown("Visualize clustering results t·ª´ project2")
    
    sample_data, data_error = load_sample_data()
    
    if data_error:
        st.error(data_error)
    else:
        st.info(f"üìä ƒêang load {len(sample_data)} records t·ª´ d·ªØ li·ªáu")
        
        # Import clustering functions
        from sklearn.cluster import KMeans, AgglomerativeClustering
        from sklearn.mixture import GaussianMixture
        from sklearn.preprocessing import StandardScaler
        from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        tab1, tab2, tab3 = st.tabs(["üîç Clustering", "üìä Content-Based Filtering", "üìà Visualization"])
        
        with tab1:
            st.subheader("üîç Ph√¢n c·ª•m d·ªØ li·ªáu")
            
            # Prepare data for clustering
            @st.cache_data
            def prepare_clustering_data(df):
                """Prepare numeric features for clustering"""
                try:
                    if df is None or len(df) == 0:
                        return None, None, None
                    
                    df_clean = df.copy()
                    numeric_cols = []
                    
                    # Parse price
                    from utils import parse_price
                    if 'Gi√°' in df_clean.columns:
                        df_clean['price_parsed'] = df_clean['Gi√°'].apply(lambda x: parse_price(x) if pd.notna(x) else 0)
                        numeric_cols.append('price_parsed')
                    elif 'gia_vnd' in df_clean.columns:
                        df_clean['price_parsed'] = df_clean['gia_vnd'] / 1_000_000
                        numeric_cols.append('price_parsed')
                    
                    # Year - parse carefully to handle strings like "2008 tr∆∞·ªõc nƒÉm"
                    def parse_year(value):
                        if pd.isna(value):
                            return 0
                        try:
                            # Convert to string first
                            value_str = str(value).strip()
                            # Extract first 4 digits (year)
                            import re
                            year_match = re.search(r'\d{4}', value_str)
                            if year_match:
                                year = int(year_match.group())
                                # Validate year range
                                if 1990 <= year <= 2025:
                                    return year
                            return 0
                        except:
                            return 0
                    
                    if 'NƒÉm ƒëƒÉng k√Ω' in df_clean.columns:
                        df_clean['year_parsed'] = df_clean['NƒÉm ƒëƒÉng k√Ω'].apply(parse_year)
                        numeric_cols.append('year_parsed')
                    elif 'nam_dang_ky' in df_clean.columns:
                        df_clean['year_parsed'] = df_clean['nam_dang_ky'].apply(parse_year)
                        numeric_cols.append('year_parsed')
                    
                    # KM - parse carefully
                    def parse_km(value):
                        if pd.isna(value):
                            return 0
                        try:
                            # Convert to string and extract numbers
                            value_str = str(value).strip().lower()
                            # Remove common text
                            value_str = value_str.replace('km', '').replace(',', '').replace('.', '').strip()
                            # Extract numbers
                            import re
                            numbers = re.findall(r'\d+', value_str)
                            if numbers:
                                return float(''.join(numbers))
                            return 0
                        except:
                            return 0
                    
                    if 'S·ªë Km ƒë√£ ƒëi' in df_clean.columns:
                        df_clean['km_parsed'] = df_clean['S·ªë Km ƒë√£ ƒëi'].apply(parse_km)
                        numeric_cols.append('km_parsed')
                    elif 'so_km' in df_clean.columns:
                        df_clean['km_parsed'] = df_clean['so_km'].apply(parse_km)
                        numeric_cols.append('km_parsed')
                    
                    # One-hot encode brand
                    brand_col = None
                    if 'Th∆∞∆°ng hi·ªáu' in df_clean.columns:
                        brand_col = 'Th∆∞∆°ng hi·ªáu'
                    elif 'thuong_hieu' in df_clean.columns:
                        brand_col = 'thuong_hieu'
                    
                    if brand_col and df_clean[brand_col].notna().sum() > 0:
                        # Limit to top brands to avoid too many features
                        top_brands = df_clean[brand_col].value_counts().head(10).index.tolist()
                        for brand in top_brands:
                            col_name = f'brand_{brand}'
                            df_clean[col_name] = (df_clean[brand_col] == brand).astype(int)
                            numeric_cols.append(col_name)
                    
                    # Check if we have enough features
                    if len(numeric_cols) == 0:
                        st.warning("Kh√¥ng t√¨m th·∫•y c·ªôt s·ªë ph√π h·ª£p. Th√™m c√°c c·ªôt m·∫∑c ƒë·ªãnh.")
                        # Add dummy features
                        df_clean['dummy_feature'] = 1
                        numeric_cols.append('dummy_feature')
                    
                    # Select and clean
                    available_cols = [col for col in numeric_cols if col in df_clean.columns]
                    if len(available_cols) == 0:
                        return None, None, None
                    
                    X = df_clean[available_cols].fillna(0)
                    
                    # Remove rows with all zeros
                    X = X[(X != 0).any(axis=1)]
                    if len(X) == 0:
                        return None, None, None
                    
                    # Scale
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)
                    
                    # Update df_clean to match X indices
                    df_clean = df_clean.loc[X.index].copy()
                    
                    return X_scaled, df_clean, scaler
                except Exception as e:
                    st.error(f"L·ªói khi chu·∫©n b·ªã d·ªØ li·ªáu: {str(e)}")
                    import traceback
                    st.code(traceback.format_exc())
                    return None, None, None
            
            X_scaled, df_clean, scaler = prepare_clustering_data(sample_data)
            
            if X_scaled is not None:
                col1, col2 = st.columns(2)
                
                with col1:
                    n_clusters = st.slider("S·ªë c·ª•m (k)", min_value=2, max_value=10, value=5)
                    algorithm = st.selectbox(
                        "Thu·∫≠t to√°n clustering",
                        ["KMeans", "Gaussian Mixture Model (GMM)", "Agglomerative Clustering"]
                    )
                
                with col2:
                    max_samples = st.slider("S·ªë m·∫´u t·ªëi ƒëa", min_value=100, max_value=min(1000, len(sample_data)), value=min(500, len(sample_data)))
                
                if st.button("üöÄ Ch·∫°y Clustering", use_container_width=True):
                    with st.spinner("ƒêang ch·∫°y clustering..."):
                        # Sample data if too large
                        if len(X_scaled) > max_samples:
                            indices = np.random.choice(len(X_scaled), max_samples, replace=False)
                            X_sample = X_scaled[indices]
                            df_sample = df_clean.iloc[indices].copy()
                        else:
                            X_sample = X_scaled
                            df_sample = df_clean.copy()
                        
                        # Run clustering
                        if algorithm == "KMeans":
                            model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                            labels = model.fit_predict(X_sample)
                        elif algorithm == "Gaussian Mixture Model (GMM)":
                            model = GaussianMixture(n_components=n_clusters, random_state=42)
                            labels = model.fit_predict(X_sample)
                        else:  # Agglomerative
                            model = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
                            labels = model.fit_predict(X_sample)
                        
                        # Calculate metrics
                        if len(np.unique(labels)) >= 2:
                            sil_score = silhouette_score(X_sample, labels)
                            db_score = davies_bouldin_score(X_sample, labels)
                            ch_score = calinski_harabasz_score(X_sample, labels)
                        else:
                            sil_score = db_score = ch_score = np.nan
                        
                        # Display results
                        st.success(f"‚úÖ Ho√†n th√†nh clustering v·ªõi {algorithm}")
                        
                        # Metrics
                        metric_col1, metric_col2, metric_col3 = st.columns(3)
                        with metric_col1:
                            st.metric("Silhouette Score", f"{sil_score:.4f}" if not np.isnan(sil_score) else "N/A")
                        with metric_col2:
                            st.metric("Davies-Bouldin Score", f"{db_score:.4f}" if not np.isnan(db_score) else "N/A")
                        with metric_col3:
                            st.metric("Calinski-Harabasz Score", f"{ch_score:.4f}" if not np.isnan(ch_score) else "N/A")
                        
                        # Cluster summary
                        df_sample['cluster'] = labels
                        st.subheader("üìä T√≥m t·∫Øt c√°c c·ª•m")
                        
                        cluster_summary = []
                        for cluster_id in range(n_clusters):
                            cluster_data = df_sample[df_sample['cluster'] == cluster_id]
                            if len(cluster_data) > 0:
                                price_col = 'price_parsed' if 'price_parsed' in cluster_data.columns else 'Gi√°'
                                year_col = 'NƒÉm ƒëƒÉng k√Ω' if 'NƒÉm ƒëƒÉng k√Ω' in cluster_data.columns else None
                                
                                prices = cluster_data[price_col].dropna()
                                years = cluster_data[year_col].dropna() if year_col else pd.Series()
                                
                                brand_counts = cluster_data['Th∆∞∆°ng hi·ªáu'].value_counts().head(3) if 'Th∆∞∆°ng hi·ªáu' in cluster_data.columns else {}
                                
                                cluster_summary.append({
                                    'C·ª•m': cluster_id,
                                    'S·ªë l∆∞·ª£ng': len(cluster_data),
                                    'Gi√° TB (tri·ªáu)': f"{prices.mean():.2f}" if len(prices) > 0 else "N/A",
                                    'NƒÉm TB': f"{years.mean():.0f}" if len(years) > 0 else "N/A",
                                    'Th∆∞∆°ng hi·ªáu ph·ªï bi·∫øn': ", ".join(brand_counts.index.tolist()[:3]) if len(brand_counts) > 0 else "N/A"
                                })
                        
                        summary_df = pd.DataFrame(cluster_summary)
                        st.dataframe(summary_df, use_container_width=True, hide_index=True)
                        
                        # Show samples from each cluster
                        st.subheader("üîç M·∫´u t·ª´ c√°c c·ª•m")
                        selected_cluster = st.selectbox("Ch·ªçn c·ª•m ƒë·ªÉ xem", range(n_clusters))
                        cluster_samples = df_sample[df_sample['cluster'] == selected_cluster]
                        
                        display_cols = ['Ti√™u ƒë·ªÅ', 'Gi√°', 'Th∆∞∆°ng hi·ªáu', 'NƒÉm ƒëƒÉng k√Ω']
                        available_cols = [col for col in display_cols if col in cluster_samples.columns]
                        st.dataframe(
                            cluster_samples[available_cols].head(20),
                            use_container_width=True,
                            hide_index=True
                        )
            else:
                st.error("Kh√¥ng th·ªÉ chu·∫©n b·ªã d·ªØ li·ªáu cho clustering. Ki·ªÉm tra l·∫°i d·ªØ li·ªáu.")
        
        with tab2:
            st.subheader("üìä Content-Based Filtering")
            st.markdown("T√¨m xe t∆∞∆°ng t·ª± d·ª±a tr√™n n·ªôi dung (th∆∞∆°ng hi·ªáu, gi√°, nƒÉm, m√¥ t·∫£)")
            
            if sample_data is not None and len(sample_data) > 0:
                # Select a bike
                st.markdown("### Ch·ªçn xe ƒë·ªÉ t√¨m c√°c xe t∆∞∆°ng t·ª±")
                
                if 'Ti√™u ƒë·ªÅ' in sample_data.columns:
                    bike_options = sample_data['Ti√™u ƒë·ªÅ'].head(50).tolist()
                    selected_bike_title = st.selectbox("Ch·ªçn xe", bike_options)
                    selected_bike = sample_data[sample_data['Ti√™u ƒë·ªÅ'] == selected_bike_title].iloc[0]
                else:
                    st.warning("Kh√¥ng c√≥ c·ªôt 'Ti√™u ƒë·ªÅ' trong d·ªØ li·ªáu")
                    selected_bike = None
                
                if selected_bike is not None:
                    top_n = st.slider("S·ªë xe t∆∞∆°ng t·ª±", min_value=1, max_value=20, value=5)
                    
                    if st.button("üîç T√¨m xe t∆∞∆°ng t·ª±", use_container_width=True):
                        with st.spinner("ƒêang t√≠nh to√°n similarity..."):
                            # Prepare features for content-based
                            def prepare_content_features(df):
                                features = []
                                for idx, row in df.iterrows():
                                    feature_vec = []
                                    
                                    # Brand (one-hot like)
                                    if 'Th∆∞∆°ng hi·ªáu' in row:
                                        brand = str(row['Th∆∞∆°ng hi·ªáu']).lower()
                                        feature_vec.append(hash(brand) % 1000 / 1000.0)
                                    else:
                                        feature_vec.append(0)
                                    
                                    # Price (normalized)
                                    from utils import parse_price
                                    price = parse_price(row.get('Gi√°', 0))
                                    if price:
                                        feature_vec.append(price / 100.0)  # Normalize
                                    else:
                                        feature_vec.append(0)
                                    
                                    # Year (normalized)
                                    year = row.get('NƒÉm ƒëƒÉng k√Ω', 0)
                                    if pd.notna(year) and year > 0:
                                        feature_vec.append((year - 2000) / 25.0)  # Normalize
                                    else:
                                        feature_vec.append(0)
                                    
                                    # KM (normalized)
                                    km = row.get('S·ªë Km ƒë√£ ƒëi', 0)
                                    if pd.notna(km) and km > 0:
                                        feature_vec.append(km / 100000.0)  # Normalize
                                    else:
                                        feature_vec.append(0)
                                    
                                    features.append(feature_vec)
                                
                                return np.array(features)
                            
                            # Get features for all bikes
                            all_features = prepare_content_features(sample_data)
                            selected_idx = sample_data[sample_data['Ti√™u ƒë·ªÅ'] == selected_bike_title].index[0]
                            selected_features = all_features[selected_idx:selected_idx+1]
                            
                            # Calculate cosine similarity
                            similarities = cosine_similarity(selected_features, all_features)[0]
                            
                            # Get top N similar (exclude itself)
                            similar_indices = np.argsort(similarities)[::-1][1:top_n+1]
                            
                            # Display results
                            st.success(f"‚úÖ T√¨m th·∫•y {len(similar_indices)} xe t∆∞∆°ng t·ª±")
                            
                            for i, idx in enumerate(similar_indices, 1):
                                similar_bike = sample_data.iloc[idx]
                                similarity = similarities[idx]
                                
                                with st.container():
                                    cols = st.columns([1, 3, 1, 1])
                                    with cols[0]:
                                        st.write(f"**#{i}**")
                                        st.progress(similarity)
                                    with cols[1]:
                                        title = similar_bike.get('Ti√™u ƒë·ªÅ', 'N/A')
                                        st.write(f"**{title}**")
                                    with cols[2]:
                                        from utils import format_price, parse_price
                                        price = parse_price(similar_bike.get('Gi√°', 0))
                                        st.write(format_price(price))
                                    with cols[3]:
                                        st.write(f"Similarity: {similarity:.3f}")
                                    st.divider()
        
        with tab3:
            st.subheader("üìà Visualization")
            st.markdown("Bi·ªÉu ƒë·ªì ph√¢n c·ª•m (c·∫ßn ch·∫°y clustering tr∆∞·ªõc)")
            
            if 'cluster' in st.session_state:
                st.info("T√≠nh nƒÉng visualization ƒëang ph√°t tri·ªÉn. S·∫Ω hi·ªÉn th·ªã bi·ªÉu ƒë·ªì ph√¢n c·ª•m 2D/3D.")
            else:
                st.info("üí° Ch·∫°y clustering ·ªü tab 'Clustering' tr∆∞·ªõc ƒë·ªÉ xem visualization")

# Footer
st.sidebar.markdown("---")
st.sidebar.markdown("### üìö T√†i li·ªáu")
st.sidebar.markdown("[GitHub Repository](https://github.com/teddyDn2001/ProjectChoTot)")
st.sidebar.markdown("[README](README.md)")

